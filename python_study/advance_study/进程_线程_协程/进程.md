# Python 进程详解：从基础到高级

## 1. 进程简介

进程（Process）是操作系统进行资源分配和调度的基本单位，是程序执行时的一个实例。每个进程都有自己独立的内存空间、文件描述符和系统资源，进程间相互隔离，需要通过特定的进程间通信（IPC）机制进行数据交换。

Python 通过 `multiprocessing` 模块提供了强大的多进程支持，可以充分利用多核 CPU 的优势，特别适合计算密集型任务。

### 1.1 为什么需要多进程？

1. **突破 GIL 限制**：Python 的全局解释器锁（GIL）限制了多线程并行执行 CPU 密集型任务，多进程可以绕过此限制
2. **真正的并行计算**：多个进程可以在不同的 CPU 核心上同时执行
3. **更好的资源隔离**：进程崩溃不会影响其他进程，提高了程序的稳定性
4. **充分利用多核 CPU**：现代计算机大多是多核处理器，多进程可以充分利用硬件资源

## 2. 进程 vs 线程 vs 协程

| 特性 | 进程 | 线程 | 协程 |
|------|------|------|------|
| **创建开销** | 大（需要复制内存空间） | 小（共享进程内存） | 极小（用户态调度） |
| **切换开销** | 大（上下文切换） | 中等 | 极小 |
| **内存占用** | 大（独立内存空间） | 小（共享内存） | 极小 |
| **数据共享** | 需要 IPC 机制 | 共享内存（需同步） | 共享内存 |
| **并行性** | 多核并行 | 受 GIL 限制 | 单线程并发 |
| **稳定性** | 高（隔离性好） | 中（相互影响） | 高 |
| **适用场景** | 计算密集型 | I/O 密集型 | I/O 密集型 |
| **通信成本** | 高（IPC） | 低（共享内存） | 低（直接调用） |

## 3. 创建进程的多种方式

### 3.1 使用 `multiprocessing.Process`

这是最常用的创建进程方式：

```python
import multiprocessing
import os
import time

def worker(name, delay):
    """工作进程函数"""
    print(f"Process {name} (PID: {os.getpid()}) started")
    time.sleep(delay)
    print(f"Process {name} finished")
    return f"Result from {name}"

if __name__ == "__main__":
    # 创建进程
    p1 = multiprocessing.Process(target=worker, args=("A", 2))
    p2 = multiprocessing.Process(target=worker, args=("B", 1))
    
    # 启动进程
    p1.start()
    p2.start()
    
    # 等待进程结束
    p1.join()
    p2.join()
    
    print("All processes completed")
```

### 3.2 继承 `multiprocessing.Process` 类

创建自定义进程类：

```python
import multiprocessing
import time

class WorkerProcess(multiprocessing.Process):
    def __init__(self, name, delay):
        super().__init__()
        self.name = name
        self.delay = delay
    
    def run(self):
        """重写 run 方法"""
        print(f"Worker {self.name} started")
        time.sleep(self.delay)
        print(f"Worker {self.name} finished")
        return f"Result from {self.name}"

if __name__ == "__main__":
    processes = []
    
    # 创建并启动多个进程
    for i in range(3):
        p = WorkerProcess(f"Worker-{i}", i+1)
        processes.append(p)
        p.start()
    
    # 等待所有进程完成
    for p in processes:
        p.join()
    
    print("All workers completed")
```

### 3.3 使用进程池（Process Pool）

对于需要执行大量任务的情况，使用进程池更高效：

```python
import multiprocessing
import time

def square(x):
    """计算平方"""
    time.sleep(0.5)  # 模拟计算耗时
    return x * x

if __name__ == "__main__":
    # 创建进程池（默认使用 CPU 核心数）
    with multiprocessing.Pool() as pool:
        # 同步执行
        result = pool.apply(square, (5,))
        print(f"apply result: {result}")
        
        # 异步执行
        async_result = pool.apply_async(square, (10,))
        print(f"apply_async result: {async_result.get()}")
        
        # 批量处理（map）
        numbers = list(range(1, 11))
        results = pool.map(square, numbers)
        print(f"map results: {results}")
        
        # 异步批量处理（map_async）
        async_results = pool.map_async(square, numbers)
        print(f"map_async results: {async_results.get()}")
        
        # 无序批量处理（imap_unordered）
        for result in pool.imap_unordered(square, numbers):
            print(f"imap_unordered: {result}")
```

### 3.4 使用 `concurrent.futures.ProcessPoolExecutor`

Python 3.2+ 提供了更高级的进程池 API：

```python
import concurrent.futures
import time

def task(n):
    time.sleep(1)
    return n * n

if __name__ == "__main__":
    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
        # 提交单个任务
        future = executor.submit(task, 10)
        print(f"Future result: {future.result()}")
        
        # 批量提交任务
        futures = [executor.submit(task, i) for i in range(5)]
        
        # 按完成顺序获取结果
        for future in concurrent.futures.as_completed(futures):
            print(f"Completed: {future.result()}")
        
        # 使用 map
        results = list(executor.map(task, range(1, 6)))
        print(f"Map results: {results}")
```

### 3.5 Unix/Linux 系统的 `os.fork()`

在 Unix-like 系统上可以使用 `os.fork()`：

```python
import os
import time

def fork_example():
    pid = os.fork()
    
    if pid == 0:
        # 子进程
        print(f"Child process (PID: {os.getpid()})")
        time.sleep(1)
        print("Child process exiting")
        os._exit(0)
    else:
        # 父进程
        print(f"Parent process (PID: {os.getpid()}), created child {pid}")
        os.waitpid(pid, 0)  # 等待子进程结束
        print("Parent process exiting")

if __name__ == "__main__":
    fork_example()
```

**注意**：`os.fork()` 在 Windows 上不可用，跨平台程序应使用 `multiprocessing`。

## 4. 进程间通信（IPC）

由于进程间内存相互隔离，需要通过特定的 IPC 机制进行数据交换。

### 4.1 队列（Queue）

`multiprocessing.Queue` 是进程安全的队列，支持多生产者和多消费者：

```python
import multiprocessing
import time
import random

def producer(queue, name):
    """生产者进程"""
    for i in range(5):
        item = f"Item-{name}-{i}"
        time.sleep(random.uniform(0.1, 0.5))
        queue.put(item)
        print(f"Producer {name} produced: {item}")
    queue.put(None)  # 发送结束信号

def consumer(queue, name):
    """消费者进程"""
    while True:
        item = queue.get()
        if item is None:
            queue.put(None)  # 传递给其他消费者
            break
        time.sleep(random.uniform(0.2, 0.8))
        print(f"Consumer {name} consumed: {item}")

if __name__ == "__main__":
    queue = multiprocessing.Queue(maxsize=10)
    
    # 创建生产者和消费者
    producers = [
        multiprocessing.Process(target=producer, args=(queue, f"P{i}"))
        for i in range(2)
    ]
    
    consumers = [
        multiprocessing.Process(target=consumer, args=(queue, f"C{i}"))
        for i in range(3)
    ]
    
    # 启动所有进程
    for p in producers + consumers:
        p.start()
    
    # 等待生产者完成
    for p in producers:
        p.join()
    
    # 发送结束信号给消费者
    for _ in consumers:
        queue.put(None)
    
    # 等待消费者完成
    for c in consumers:
        c.join()
    
    print("All processes completed")
```

### 4.2 管道（Pipe）

管道提供双向通信：

```python
import multiprocessing
import time

def worker(conn):
    """工作进程"""
    # 接收数据
    data = conn.recv()
    print(f"Worker received: {data}")
    
    # 处理数据
    result = data.upper()
    
    # 发送结果
    time.sleep(1)
    conn.send(result)
    conn.close()

if __name__ == "__main__":
    # 创建管道（返回两个连接对象）
    parent_conn, child_conn = multiprocessing.Pipe()
    
    # 创建子进程
    p = multiprocessing.Process(target=worker, args=(child_conn,))
    p.start()
    
    # 父进程发送数据
    parent_conn.send("hello from parent")
    
    # 接收结果
    result = parent_conn.recv()
    print(f"Parent received: {result}")
    
    p.join()
```

### 4.3 共享内存

#### 4.3.1 `Value` 和 `Array`

`multiprocessing.Value` 和 `multiprocessing.Array` 提供进程间共享内存：

```python
import multiprocessing
import time

def increment_counter(counter):
    """递增计数器"""
    for _ in range(100000):
        with counter.get_lock():  # 使用锁保证原子性
            counter.value += 1

def modify_array(arr, index):
    """修改共享数组"""
    for i in range(1000):
        arr[index] += 1

if __name__ == "__main__":
    # 共享值（带锁）
    counter = multiprocessing.Value('i', 0)
    
    # 共享数组
    arr = multiprocessing.Array('i', [0, 0, 0, 0, 0])
    
    # 创建进程
    processes = []
    
    # 计数器进程
    for _ in range(4):
        p = multiprocessing.Process(target=increment_counter, args=(counter,))
        processes.append(p)
        p.start()
    
    # 数组进程
    for i in range(5):
        p = multiprocessing.Process(target=modify_array, args=(arr, i))
        processes.append(p)
        p.start()
    
    # 等待所有进程
    for p in processes:
        p.join()
    
    print(f"Counter final value: {counter.value}")
    print(f"Array final values: {list(arr)}")
```

#### 4.3.2 `multiprocessing.shared_memory`（Python 3.8+）

Python 3.8 引入了更灵活的共享内存模块：

```python
import multiprocessing
import time
from multiprocessing import shared_memory

def worker(shm_name, index):
    """工作进程，通过共享内存通信"""
    # 连接到现有共享内存
    existing_shm = shared_memory.SharedMemory(name=shm_name)
    
    # 将共享内存视为数组
    import array
    arr = array.array('i', existing_shm.buf)
    
    # 修改数据
    for i in range(1000):
        arr[index] += 1
    
    # 关闭连接
    existing_shm.close()

if __name__ == "__main__":
    # 创建共享内存
    shm = shared_memory.SharedMemory(create=True, size=20)  # 20字节
    
    # 初始化共享内存
    import array
    arr = array.array('i', [0, 0, 0, 0])
    arr_bytes = arr.tobytes()
    shm.buf[:len(arr_bytes)] = arr_bytes
    
    # 创建进程
    processes = []
    for i in range(4):
        p = multiprocessing.Process(target=worker, args=(shm.name, i))
        processes.append(p)
        p.start()
    
    # 等待进程
    for p in processes:
        p.join()
    
    # 读取最终结果
    result = array.array('i', shm.buf[:16])
    print(f"Final array: {list(result)}")
    
    # 清理共享内存
    shm.close()
    shm.unlink()
```

### 4.4 管理器（Manager）

`multiprocessing.Manager` 可以创建在进程间共享的 Python 对象：

```python
import multiprocessing
import time

def modify_dict(shared_dict, key):
    """修改共享字典"""
    for i in range(10):
        if key not in shared_dict:
            shared_dict[key] = 0
        shared_dict[key] += 1
        time.sleep(0.01)

def modify_list(shared_list, value):
    """修改共享列表"""
    for i in range(5):
        shared_list.append(f"{value}-{i}")
        time.sleep(0.02)

if __name__ == "__main__":
    with multiprocessing.Manager() as manager:
        # 创建共享数据结构
        shared_dict = manager.dict()
        shared_list = manager.list()
        
        # 创建进程
        processes = []
        
        # 字典进程
        for i in range(3):
            p = multiprocessing.Process(target=modify_dict, args=(shared_dict, f"key{i}"))
            processes.append(p)
            p.start()
        
        # 列表进程
        for i in range(2):
            p = multiprocessing.Process(target=modify_list, args=(shared_list, f"worker{i}"))
            processes.append(p)
            p.start()
        
        # 等待进程
        for p in processes:
            p.join()
        
        print(f"Final dict: {dict(shared_dict)}")
        print(f"Final list: {list(shared_list)}")
```

## 5. 进程同步机制

### 5.1 锁（Lock）

```python
import multiprocessing
import time

def worker_with_lock(lock, counter, worker_id):
    """使用锁的工作进程"""
    for _ in range(5):
        # 获取锁
        lock.acquire()
        try:
            # 临界区代码
            current = counter.value
            time.sleep(0.1)  # 模拟工作
            counter.value = current + 1
            print(f"Worker {worker_id} incremented counter to {counter.value}")
        finally:
            # 释放锁
            lock.release()
        time.sleep(0.5)

if __name__ == "__main__":
    lock = multiprocessing.Lock()
    counter = multiprocessing.Value('i', 0)
    
    processes = []
    for i in range(3):
        p = multiprocessing.Process(target=worker_with_lock, args=(lock, counter, i))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
    
    print(f"Final counter value: {counter.value}")
```

### 5.2 信号量（Semaphore）

```python
import multiprocessing
import time
import random

def worker_with_semaphore(semaphore, worker_id):
    """使用信号量的工作进程"""
    for i in range(3):
        # 获取信号量
        semaphore.acquire()
        try:
            print(f"Worker {worker_id} started task {i}")
            time.sleep(random.uniform(0.5, 1.5))
            print(f"Worker {worker_id} finished task {i}")
        finally:
            # 释放信号量
            semaphore.release()
        time.sleep(0.5)

if __name__ == "__main__":
    # 最多允许2个进程同时执行
    semaphore = multiprocessing.Semaphore(2)
    
    processes = []
    for i in range(5):
        p = multiprocessing.Process(target=worker_with_semaphore, args=(semaphore, i))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
```

### 5.3 事件（Event）

```python
import multiprocessing
import time

def waiter(event, worker_id):
    """等待事件的进程"""
    print(f"Worker {worker_id} waiting for event")
    event.wait()  # 阻塞直到事件被设置
    print(f"Worker {worker_id} received event, continuing")

def setter(event):
    """设置事件的进程"""
    print("Setter sleeping for 2 seconds")
    time.sleep(2)
    print("Setter setting event")
    event.set()  # 设置事件，唤醒所有等待的进程

if __name__ == "__main__":
    event = multiprocessing.Event()
    
    # 创建等待进程
    waiters = []
    for i in range(3):
        p = multiprocessing.Process(target=waiter, args=(event, i))
        waiters.append(p)
        p.start()
    
    # 创建设置进程
    setter_proc = multiprocessing.Process(target=setter, args=(event,))
    setter_proc.start()
    
    # 等待所有进程
    for p in waiters:
        p.join()
    setter_proc.join()
```

### 5.4 条件变量（Condition）

```python
import multiprocessing
import time
import random

def producer(condition, queue, producer_id):
    """生产者进程"""
    for i in range(3):
        time.sleep(random.uniform(0.5, 1.0))
        
        with condition:
            item = f"Item-{producer_id}-{i}"
            queue.append(item)
            print(f"Producer {producer_id} produced: {item}")
            
            # 通知消费者
            condition.notify_all()

def consumer(condition, queue, consumer_id):
    """消费者进程"""
    while True:
        with condition:
            # 等待队列中有数据
            while len(queue) == 0:
                condition.wait()
            
            # 消费数据
            item = queue.pop(0)
            print(f"Consumer {consumer_id} consumed: {item}")
            time.sleep(random.uniform(0.2, 0.5))

if __name__ == "__main__":
    manager = multiprocessing.Manager()
    queue = manager.list()
    condition = multiprocessing.Condition()
    
    # 创建进程
    producers = []
    consumers = []
    
    for i in range(2):
        p = multiprocessing.Process(target=producer, args=(condition, queue, i))
        producers.append(p)
        p.start()
    
    for i in range(3):
        p = multiprocessing.Process(target=consumer, args=(condition, queue, i))
        consumers.append(p)
        p.start()
    
    # 等待生产者完成
    for p in producers:
        p.join()
    
    # 给消费者一些时间处理剩余数据
    time.sleep(2)
    
    # 终止消费者
    for p in consumers:
        p.terminate()
        p.join()
```

### 5.5 屏障（Barrier）

```python
import multiprocessing
import time
import random

def worker(barrier, worker_id):
    """使用屏障的工作进程"""
    print(f"Worker {worker_id} started phase 1")
    time.sleep(random.uniform(0.5, 1.5))
    print(f"Worker {worker_id} finished phase 1, waiting at barrier")
    
    # 等待所有进程到达屏障
    barrier.wait()
    
    print(f"Worker {worker_id} started phase 2")
    time.sleep(random.uniform(0.5, 1.5))
    print(f"Worker {worker_id} finished phase 2")

if __name__ == "__main__":
    # 创建屏障，需要3个进程到达后才能继续
    barrier = multiprocessing.Barrier(3)
    
    processes = []
    for i in range(3):
        p = multiprocessing.Process(target=worker, args=(barrier, i))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join()
```

## 6. 进程池高级用法

### 6.1 自定义进程池大小

```python
import multiprocessing
import os
import time

def cpu_intensive_task(n):
    """CPU 密集型任务"""
    result = 0
    for i in range(n * 1000000):
        result += i % 256
    return result, os.getpid()

if __name__ == "__main__":
    # 获取 CPU 核心数
    cpu_count = multiprocessing.cpu_count()
    print(f"CPU cores: {cpu_count}")
    
    # 根据任务类型选择进程数
    # I/O 密集型：可以设置更多进程
    # CPU 密集型：最好设置为 CPU 核心数
    pool_size = cpu_count
    
    with multiprocessing.Pool(pool_size) as pool:
        # 执行批量任务
        tasks = [100, 200, 300, 400, 500]
        results = pool.map(cpu_intensive_task, tasks)
        
        for result, pid in results:
            print(f"Result: {result}, PID: {pid}")
```

### 6.2 进程池中的异常处理

```python
import multiprocessing
import traceback

def task_with_exception(x):
    """可能抛出异常的任务"""
    if x == 3:
        raise ValueError(f"Invalid value: {x}")
    return x * x

if __name__ == "__main__":
    with multiprocessing.Pool(2) as pool:
        # 方法1：使用 try-except
        try:
            result = pool.apply(task_with_exception, (3,))
        except Exception as e:
            print(f"Caught exception: {e}")
        
        # 方法2：使用 map 和错误处理
        numbers = [1, 2, 3, 4, 5]
        
        # 逐个提交并处理异常
        async_results = []
        for n in numbers:
            async_result = pool.apply_async(task_with_exception, (n,))
            async_results.append(async_result)
        
        for i, async_result in enumerate(async_results):
            try:
                result = async_result.get()
                print(f"Task {i} succeeded: {result}")
            except Exception as e:
                print(f"Task {i} failed: {e}")
                traceback.print_exc()
```

### 6.3 进程池回调函数

```python
import multiprocessing
import time

def process_task(n):
    """处理任务"""
    time.sleep(0.5)
    return n * n

def callback_success(result):
    """成功回调"""
    print(f"Task succeeded with result: {result}")

def callback_error(error):
    """错误回调"""
    print(f"Task failed with error: {error}")

if __name__ == "__main__":
    with multiprocessing.Pool(2) as pool:
        # 使用 apply_async 并指定回调
        for i in range(5):
            pool.apply_async(
                process_task,
                args=(i,),
                callback=callback_success,
                error_callback=callback_error
            )
        
        # 等待所有任务完成
        pool.close()
        pool.join()
```

### 6.4 进程池的初始化函数

```python
import multiprocessing
import time

# 全局变量，需要在每个进程中初始化
process_local_data = None

def init_process():
    """进程初始化函数"""
    global process_local_data
    process_local_data = {
        'pid': multiprocessing.current_process().pid,
        'start_time': time.time()
    }
    print(f"Process {process_local_data['pid']} initialized")

def worker_task(x):
    """工作任务，使用进程局部数据"""
    global process_local_data
    time.sleep(0.1)
    return x * x, process_local_data['pid']

if __name__ == "__main__":
    # 创建进程池并指定初始化函数
    with multiprocessing.Pool(initializer=init_process, initargs=()) as pool:
        results = pool.map(worker_task, range(10))
        
        for result, pid in results:
            print(f"Result: {result}, PID: {pid}")
```

## 7. 进程监控和管理

### 7.1 获取进程信息

```python
import multiprocessing
import os
import time
import psutil  # 需要安装：pip install psutil

def worker():
    """工作进程"""
    print(f"Worker PID: {os.getpid()}, Parent PID: {os.getppid()}")
    time.sleep(10)

def monitor_processes(processes):
    """监控进程状态"""
    while any(p.is_alive() for p in processes):
        for i, p in enumerate(processes):
            if p.is_alive():
                try:
                    # 使用 psutil 获取详细信息
                    proc = psutil.Process(p.pid)
                    cpu_percent = proc.cpu_percent(interval=0.1)
                    memory_info = proc.memory_info()
                    
                    print(f"Process {i} (PID: {p.pid}): "
                          f"CPU: {cpu_percent}%, "
                          f"Memory: {memory_info.rss / 1024 / 1024:.2f} MB")
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
        
        time.sleep(1)

if __name__ == "__main__":
    # 创建进程
    processes = []
    for i in range(3):
        p = multiprocessing.Process(target=worker)
        processes.append(p)
        p.start()
    
    # 监控进程
    try:
        monitor_processes(processes)
    except KeyboardInterrupt:
        print("\nInterrupted, terminating processes...")
        for p in processes:
            p.terminate()
    
    # 等待进程结束
    for p in processes:
        p.join()
    
    print("All processes terminated")
```

### 7.2 进程生命周期管理

```python
import multiprocessing
import signal
import time
import sys

def graceful_shutdown(signum, frame):
    """优雅关闭信号处理"""
    print(f"Received signal {signum}, shutting down...")
    sys.exit(0)

def worker_with_cleanup():
    """带有清理逻辑的工作进程"""
    # 设置信号处理
    signal.signal(signal.SIGTERM, graceful_shutdown)
    signal.signal(signal.SIGINT, graceful_shutdown)
    
    try:
        print(f"Worker {os.getpid()} started")
        
        # 模拟工作
        for i in range(10):
            print(f"Worker {os.getpid()} working... {i}")
            time.sleep(1)
        
        print(f"Worker {os.getpid()} finished normally")
    except SystemExit:
        print(f"Worker {os.getpid()} exiting gracefully")
        # 执行清理操作
        # cleanup_resources()
    except Exception as e:
        print(f"Worker {os.getpid()} error: {e}")
    finally:
        print(f"Worker {os.getpid()} cleanup completed")

if __name__ == "__main__":
    import os
    
    processes = []
    
    # 创建进程
    for i in range(2):
        p = multiprocessing.Process(target=worker_with_cleanup)
        processes.append(p)
        p.start()
    
    # 等待一段时间
    time.sleep(3)
    
    # 发送终止信号
    print("Sending termination signal...")
    for p in processes:
        p.terminate()  # 发送 SIGTERM
    
    # 等待进程结束
    for p in processes:
        p.join(timeout=5)  # 最多等待5秒
        if p.is_alive():
            print(f"Process {p.pid} did not terminate, killing...")
            p.kill()  # 强制终止
    
    print("All processes terminated")
```

## 8. 实际应用示例

### 8.1 并行计算 Pi 值

```python
import multiprocessing
import random
import math

def calculate_pi_part(samples):
    """使用蒙特卡洛方法计算 pi 的一部分"""
    inside = 0
    for _ in range(samples):
        x = random.random()
        y = random.random()
        if x*x + y*y <= 1.0:
            inside += 1
    return inside

def parallel_pi(total_samples, num_processes):
    """并行计算 Pi 值"""
    samples_per_process = total_samples // num_processes
    
    with multiprocessing.Pool(num_processes) as pool:
        # 并行计算各部分
        results = pool.map(calculate_pi_part, [samples_per_process] * num_processes)
    
    # 合并结果
    total_inside = sum(results)
    pi_estimate = 4.0 * total_inside / total_samples
    
    return pi_estimate

if __name__ == "__main__":
    total_samples = 1000000
    num_processes = multiprocessing.cpu_count()
    
    print(f"Calculating Pi with {total_samples} samples using {num_processes} processes")
    
    pi_estimate = parallel_pi(total_samples, num_processes)
    
    print(f"Estimated Pi: {pi_estimate}")
    print(f"Actual Pi:    {math.pi}")
    print(f"Error:        {abs(pi_estimate - math.pi)}")
```

### 8.2 并行图像处理

```python
import multiprocessing
from PIL import Image
import os

def process_image_chunk(args):
    """处理图像块"""
    chunk, operation, params = args
    result_chunk = chunk.copy()
    
    if operation == "grayscale":
        # 转换为灰度
        for x in range(chunk.width):
            for y in range(chunk.height):
                r, g, b = chunk.getpixel((x, y))
                gray = int(0.299 * r + 0.587 * g + 0.114 * b)
                result_chunk.putpixel((x, y), (gray, gray, gray))
    
    elif operation == "invert":
        # 颜色反转
        for x in range(chunk.width):
            for y in range(chunk.height):
                r, g, b = chunk.getpixel((x, y))
                result_chunk.putpixel((x, y), (255 - r, 255 - g, 255 - b))
    
    elif operation == "brightness":
        # 调整亮度
        factor = params.get('factor', 1.5)
        for x in range(chunk.width):
            for y in range(chunk.height):
                r, g, b = chunk.getpixel((x, y))
                r = min(255, int(r * factor))
                g = min(255, int(g * factor))
                b = min(255, int(b * factor))
                result_chunk.putpixel((x, y), (r, g, b))
    
    return result_chunk

def parallel_image_processing(image_path, operation, num_processes=4):
    """并行图像处理"""
    # 加载图像
    image = Image.open(image_path)
    width, height = image.size
    
    # 将图像分割为块
    chunk_height = height // num_processes
    chunks = []
    
    for i in range(num_processes):
        top = i * chunk_height
        bottom = (i + 1) * chunk_height if i < num_processes - 1 else height
        chunk = image.crop((0, top, width, bottom))
        chunks.append((chunk, operation, {}))
    
    # 并行处理
    with multiprocessing.Pool(num_processes) as pool:
        processed_chunks = pool.map(process_image_chunk, chunks)
    
    # 合并结果
    result_image = Image.new('RGB', (width, height))
    current_height = 0
    
    for chunk in processed_chunks:
        result_image.paste(chunk, (0, current_height))
        current_height += chunk.height
    
    return result_image

if __name__ == "__main__":
    # 使用示例
    input_image = "input.jpg"  # 输入图像路径
    output_image = "output.jpg"  # 输出图像路径
    
    if os.path.exists(input_image):
        processed = parallel_image_processing(input_image, "grayscale", 4)
        processed.save(output_image)
        print(f"Image processed and saved to {output_image}")
    else:
        print(f"Input image {input_image} not found")
```

### 8.3 并行网络爬虫

```python
import multiprocessing
import requests
import time
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import concurrent.futures

def fetch_url(url, timeout=10):
    """获取单个 URL 的内容"""
    try:
        response = requests.get(url, timeout=timeout)
        response.raise_for_status()
        return url, response.text, None
    except Exception as e:
        return url, None, str(e)

def extract_links(html, base_url):
    """从 HTML 中提取链接"""
    soup = BeautifulSoup(html, 'html.parser')
    links = set()
    
    for link in soup.find_all('a', href=True):
        href = link['href']
        absolute_url = urljoin(base_url, href)
        links.add(absolute_url)
    
    return links

def crawl_worker(queue, visited, lock, base_domain, max_depth):
    """爬虫工作进程"""
    while True:
        try:
            # 从队列获取任务
            url, depth = queue.get(timeout=5)
            
            # 检查深度限制
            if depth > max_depth:
                queue.task_done()
                continue
            
            # 检查是否已访问
            with lock:
                if url in visited:
                    queue.task_done()
                    continue
                visited.add(url)
            
            print(f"Crawling: {url} (depth: {depth})")
            
            # 获取页面内容
            url, html, error = fetch_url(url)
            
            if html:
                # 提取链接
                links = extract_links(html, url)
                
                # 将新链接加入队列
                for link in links:
                    if base_domain in link:  # 限制在同一域名内
                        queue.put((link, depth + 1))
                
                # 处理页面内容
                process_page(url, html)
            
            queue.task_done()
            
        except multiprocessing.queues.Empty:
            break
        except Exception as e:
            print(f"Error in worker: {e}")
            queue.task_done()

def process_page(url, html):
    """处理页面内容（示例）"""
    soup = BeautifulSoup(html, 'html.parser')
    title = soup.title.string if soup.title else "No title"
    print(f"Processed: {url} - {title[:50]}...")

def parallel_crawler(start_url, max_workers=4, max_depth=2):
    """并行网络爬虫"""
    manager = multiprocessing.Manager()
    queue = manager.Queue()
    visited = manager.set()
    lock = manager.Lock()
    
    # 添加起始 URL
    base_domain = start_url.split('/')[2]
    queue.put((start_url, 0))
    
    # 创建工作进程
    processes = []
    for i in range(max_workers):
        p = multiprocessing.Process(
            target=crawl_worker,
            args=(queue, visited, lock, base_domain, max_depth)
        )
        processes.append(p)
        p.start()
    
    # 等待队列清空
    queue.join()
    
    # 终止工作进程
    for p in processes:
        p.terminate()
        p.join()
    
    print(f"Crawling completed. Visited {len(visited)} pages.")

if __name__ == "__main__":
    # 使用示例
    start_url = "https://example.com"
    parallel_crawler(start_url, max_workers=4, max_depth=2)
```

## 9. 高级主题

### 9.1 进程与线程混合使用

```python
import multiprocessing
import threading
import time
import queue

def io_worker(thread_id, task_queue, result_queue):
    """I/O 密集型工作线程"""
    while True:
        try:
            task = task_queue.get(timeout=1)
            # 模拟 I/O 操作
            time.sleep(0.5)
            result = f"Thread {thread_id} processed: {task}"
            result_queue.put(result)
            task_queue.task_done()
        except queue.Empty:
            break

def cpu_worker(process_id, task_queue, result_queue):
    """CPU 密集型工作进程（包含多个线程）"""
    # 创建线程池处理 I/O 任务
    thread_count = 4
    threads = []
    local_task_queue = queue.Queue()
    local_result_queue = queue.Queue()
    
    # 创建线程
    for i in range(thread_count):
        t = threading.Thread(
            target=io_worker,
            args=(f"P{process_id}-T{i}", local_task_queue, local_result_queue)
        )
        threads.append(t)
        t.start()
    
    # 处理任务
    while True:
        try:
            # 从进程间队列获取任务
            task = task_queue.get(timeout=5)
            
            # 将任务分发给线程
            local_task_queue.put(task)
            
            # 收集线程结果
            thread_result = local_result_queue.get()
            
            # CPU 密集型计算
            time.sleep(0.1)  # 模拟计算
            
            # 将结果放回进程间队列
            final_result = f"Process {process_id}: {thread_result}"
            result_queue.put(final_result)
            
            task_queue.task_done()
            
        except queue.Empty:
            break
    
    # 等待线程完成
    for t in threads:
        t.join()

if __name__ == "__main__":
    # 创建进程间队列
    manager = multiprocessing.Manager()
    task_queue = manager.Queue()
    result_queue = manager.Queue()
    
    # 添加任务
    for i in range(20):
        task_queue.put(f"Task-{i}")
    
    # 创建进程
    processes = []
    for i in range(multiprocessing.cpu_count()):
        p = multiprocessing.Process(
            target=cpu_worker,
            args=(i, task_queue, result_queue)
        )
        processes.append(p)
        p.start()
    
    # 等待任务完成
    task_queue.join()
    
    # 收集结果
    results = []
    while not result_queue.empty():
        results.append(result_queue.get())
    
    # 终止进程
    for p in processes:
        p.terminate()
        p.join()
    
    print(f"Processed {len(results)} tasks")
    for result in results[:5]:  # 显示前5个结果
        print(result)
```

### 9.2 进程池的动态调整

```python
import multiprocessing
import time
import random
from concurrent.futures import ProcessPoolExecutor

def adaptive_worker(task_id, complexity):
    """自适应工作函数"""
    # 根据任务复杂度调整工作时间
    work_time = complexity * random.uniform(0.1, 0.5)
    time.sleep(work_time)
    
    # 模拟可能失败的任务
    if random.random() < 0.1:
        raise ValueError(f"Task {task_id} failed randomly")
    
    return f"Task {task_id} completed in {work_time:.2f}s"

def dynamic_process_pool(tasks, initial_workers=2, max_workers=8):
    """动态调整进程池大小"""
    results = []
    failed_tasks = []
    
    with ProcessPoolExecutor(max_workers=initial_workers) as executor:
        # 提交初始任务
        futures = {}
        for task_id, complexity in tasks[:initial_workers*2]:
            future = executor.submit(adaptive_worker, task_id, complexity)
            futures[future] = (task_id, complexity)
        
        task_index = initial_workers * 2
        
        while futures:
            # 等待任意任务完成
            done, _ = concurrent.futures.wait(
                futures.keys(),
                timeout=0.1,
                return_when=concurrent.futures.FIRST_COMPLETED
            )
            
            # 处理完成的任务
            for future in done:
                task_id, complexity = futures.pop(future)
                
                try:
                    result = future.result()
                    results.append(result)
                    print(f"Success: {result}")
                except Exception as e:
                    failed_tasks.append((task_id, complexity))
                    print(f"Failed: Task {task_id}, error: {e}")
            
            # 动态调整：如果有失败任务，增加工作进程
            current_workers = executor._max_workers
            if len(failed_tasks) > len(results) / 2 and current_workers < max_workers:
                new_workers = min(current_workers + 1, max_workers)
                print(f"Increasing workers from {current_workers} to {new_workers}")
                # 注意：ProcessPoolExecutor 不支持动态调整 max_workers
                # 实际项目中需要使用自定义实现
            
            # 提交新任务
            while len(futures) < current_workers * 2 and task_index < len(tasks):
                task_id, complexity = tasks[task_index]
                future = executor.submit(adaptive_worker, task_id, complexity)
                futures[future] = (task_id, complexity)
                task_index += 1
    
    return results, failed_tasks

if __name__ == "__main__":
    # 创建任务列表（任务ID, 复杂度）
    tasks = [(i, random.randint(1, 5)) for i in range(50)]
    
    print("Starting dynamic process pool...")
    results, failed = dynamic_process_pool(tasks, initial_workers=2, max_workers=4)
    
    print(f"\nCompleted: {len(results)} tasks")
    print(f"Failed: {len(failed)} tasks")
    print(f"Success rate: {len(results) / len(tasks) * 100:.1f}%")
```

### 9.3 进程间消息传递模式

```python
import multiprocessing
import time
import json

class Message:
    """消息类"""
    def __init__(self, msg_type, data, sender):
        self.type = msg_type
        self.data = data
        self.sender = sender
        self.timestamp = time.time()
    
    def to_dict(self):
        return {
            'type': self.type,
            'data': self.data,
            'sender': self.sender,
            'timestamp': self.timestamp
        }
    
    @classmethod
    def from_dict(cls, data):
        msg = cls(data['type'], data['data'], data['sender'])
        msg.timestamp = data['timestamp']
        return msg

def publisher(pub_socket, name, interval=1):
    """发布者进程"""
    import zmq  # 需要安装：pip install pyzmq
    
    context = zmq.Context()
    socket = context.socket(zmq.PUB)
    socket.connect(pub_socket)
    
    counter = 0
    while True:
        counter += 1
        message = Message(
            msg_type="data",
            data={"counter": counter, "name": name},
            sender=name
        )
        
        # 发布消息
        topic = "data"
        socket.send_multipart([
            topic.encode('utf-8'),
            json.dumps(message.to_dict()).encode('utf-8')
        ])
        
        print(f"Publisher {name} sent: {counter}")
        time.sleep(interval)

def subscriber(sub_socket, name, topics):
    """订阅者进程"""
    import zmq
    
    context = zmq.Context()
    socket = context.socket(zmq.SUB)
    socket.connect(sub_socket)
    
    # 订阅指定主题
    for topic in topics:
        socket.setsockopt_string(zmq.SUBSCRIBE, topic)
    
    while True:
        try:
            # 接收消息
            topic, message_data = socket.recv_multipart()
            topic = topic.decode('utf-8')
            
            # 解析消息
            message_dict = json.loads(message_data.decode('utf-8'))
            message = Message.from_dict(message_dict)
            
            # 处理消息
            print(f"Subscriber {name} received [{topic}]: {message.data}")
            
            # 业务逻辑处理
            process_message(message)
            
        except Exception as e:
            print(f"Subscriber {name} error: {e}")

def process_message(message):
    """处理消息的业务逻辑"""
    # 根据消息类型执行不同的处理
    if message.type == "data":
        # 处理数据消息
        pass
    elif message.type == "control":
        # 处理控制消息
        pass

def message_broker(pub_port=5556, sub_port=5557):
    """消息代理进程（ZeroMQ 代理模式）"""
    import zmq
    
    context = zmq.Context()
    
    # 前端套接字（接收发布者消息）
    frontend = context.socket(zmq.XSUB)
    frontend.bind(f"tcp://*:{pub_port}")
    
    # 后端套接字（向订阅者发送消息）
    backend = context.socket(zmq.XPUB)
    backend.bind(f"tcp://*:{sub_port}")
    
    print(f"Message broker started: pub_port={pub_port}, sub_port={sub_port}")
    
    # 代理消息
    zmq.proxy(frontend, backend)

if __name__ == "__main__":
    # 启动消息代理
    broker_process = multiprocessing.Process(
        target=message_broker,
        args=(5556, 5557)
    )
    broker_process.start()
    
    time.sleep(1)  # 等待代理启动
    
    # 启动发布者
    publishers = []
    for i in range(2):
        p = multiprocessing.Process(
            target=publisher,
            args=("tcp://localhost:5556", f"Publisher-{i}", i+1)
        )
        publishers.append(p)
        p.start()
    
    # 启动订阅者
    subscribers = []
    for i in range(3):
        p = multiprocessing.Process(
            target=subscriber,
            args=("tcp://localhost:5557", f"Subscriber-{i}", ["data"])
        )
        subscribers.append(p)
        p.start()
    
    # 运行一段时间
    time.sleep(10)
    
    # 清理
    for p in publishers + subscribers:
        p.terminate()
        p.join()
    
    broker_process.terminate()
    broker_process.join()
```

## 10. 最佳实践和注意事项

### 10.1 最佳实践

1. **合理设置进程数量**
   - CPU 密集型任务：进程数 ≤ CPU 核心数
   - I/O 密集型任务：可以设置更多进程，但避免过多导致系统负载过高
   - 使用 `multiprocessing.cpu_count()` 获取 CPU 核心数

2. **使用进程池而非大量独立进程**
   - 进程池可以复用进程，减少创建和销毁的开销
   - 更好地管理系统资源
   - 提供更高级的 API（如 map、apply_async）

3. **正确处理异常**
   - 在进程函数内部捕获和处理异常
   - 使用 `try-except` 包装可能失败的操作
   - 记录错误日志以便调试

4. **资源清理**
   - 确保进程退出时释放所有资源
   - 使用 `try-finally` 或上下文管理器
   - 对于长时间运行的进程，实现优雅退出机制

5. **避免共享状态**
   - 尽可能使用消息传递而非共享内存
   - 如果必须共享状态，使用适当的同步机制
   - 考虑使用不可变数据结构

### 10.2 常见陷阱

1. **Windows 上的 `if __name__ == "__main__"`**
   ```python
   # Windows 上必须使用这个保护
   if __name__ == "__main__":
       # 进程创建和启动代码
       pass
   ```

2. **死锁**
   - 避免嵌套获取多个锁
   - 设置锁的超时时间
   - 使用超时机制避免永久等待

3. **内存泄漏**
   - 监控进程内存使用情况
   - 及时清理不再需要的对象
   - 避免循环引用

4. **僵尸进程**
   - 确保调用 `join()` 等待子进程结束
   - 处理 `SIGCHLD` 信号（Unix）
   - 使用进程池自动管理进程生命周期

### 10.3 性能优化技巧

1. **批量处理**
   ```python
   # 不好：频繁创建进程
   for item in items:
       process = Process(target=worker, args=(item,))
       
   # 好：批量处理
   with Pool() as pool:
       results = pool.map(worker, items)
   ```

2. **减少进程间通信**
   - 通信开销很大，尽量减少数据传递
   - 使用共享内存处理大量数据
   - 考虑将任务拆分为更独立的单元

3. **选择合适的数据结构**
   - 对于只读数据，使用不可变对象
   - 使用 `array.array` 或 `numpy` 数组处理数值数据
   - 避免在进程间传递复杂对象图

4. **预热进程池**
   ```python
   # 预热进程池，避免首次执行的延迟
   with Pool() as pool:
       # 执行一些简单任务预热
       pool.map(lambda x: x, range(pool._processes))
   ```

### 10.4 调试和测试

1. **日志记录**
   ```python
   import logging
   import multiprocessing
   
   def worker():
       # 每个进程有自己的日志
       logging.basicConfig(level=logging.INFO)
       logging.info("Worker started")
   ```

2. **进程间调试**
   - 使用 `multiprocessing.log_to_stderr()` 查看内部日志
   - 设置 `loglevel=logging.DEBUG` 获取详细信息
   - 使用 `pdb` 调试子进程（需要特殊配置）

3. **单元测试**
   ```python
   import unittest
   import multiprocessing
   
   class TestMultiprocessing(unittest.TestCase):
       def test_worker(self):
           # 测试进程函数
           result = worker_function(10)
           self.assertEqual(result, 100)
   ```

## 11. 总结

Python 的多进程编程通过 `multiprocessing` 模块提供了强大的并行计算能力，特别适合 CPU 密集型任务。关键要点包括：

1. **进程创建**：使用 `Process` 类或进程池
2. **进程间通信**：队列、管道、共享内存、管理器
3. **进程同步**：锁、信号量、事件、条件变量、屏障
4. **进程池管理**：合理设置进程数，处理异常，使用回调
5. **实际应用**：并行计算、图像处理、网络爬虫等场景
6. **最佳实践**：避免常见陷阱，优化性能，正确调试

通过掌握这些知识，你可以充分利用多核 CPU 的计算能力，构建高性能的 Python 应用程序。记住，多进程不是万能的，需要根据具体场景选择合适的并发模型（进程、线程、协程）。

---

**扩展阅读**：
- [Python 官方文档 - multiprocessing](https://docs.python.org/3/library/multiprocessing.html)
- [Python 并发编程实战](https://realpython.com/python-concurrency/)
- [Effective Python - 第7章：并发与并行](https://effectivepython.com/)