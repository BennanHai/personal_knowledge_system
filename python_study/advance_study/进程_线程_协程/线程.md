# Python 线程详解：从基础到高级

## 1. 线程简介

线程（Thread）是操作系统能够进行运算调度的最小单位，是进程中的实际运作单位。一个进程可以包含多个线程，所有线程共享进程的内存空间和系统资源，但每个线程有自己的栈空间和程序计数器。

Python 通过 `threading` 模块提供了线程支持，但由于全局解释器锁（GIL）的存在，Python 线程在 CPU 密集型任务上无法实现真正的并行执行。然而，对于 I/O 密集型任务，多线程仍然可以显著提高程序性能。

### 1.1 全局解释器锁（GIL）

全局解释器锁（Global Interpreter Lock，GIL）是 CPython 解释器中的一个互斥锁，它确保在任何时刻只有一个线程执行 Python 字节码。这意味着：

1. **GIL 的作用**：保护 Python 内部数据结构（如引用计数）的线程安全
2. **GIL 的影响**：CPU 密集型多线程程序无法充分利用多核 CPU
3. **GIL 的例外**：I/O 操作、C 扩展、`numpy` 计算等可以释放 GIL
4. **绕过 GIL**：使用多进程、使用其他 Python 实现（如 Jython、IronPython）、使用 C 扩展

### 1.2 为什么需要多线程？

尽管有 GIL 的限制，多线程在以下场景仍然非常有用：

1. **I/O 密集型任务**：网络请求、文件读写、数据库操作等，线程在等待 I/O 时可以切换
2. **提高响应性**：GUI 应用程序中，使用后台线程保持界面响应
3. **简化并发模型**：某些场景下，线程比进程更轻量，编程模型更简单
4. **利用 I/O 等待时间**：当一个线程等待 I/O 时，其他线程可以继续执行

## 2. 线程 vs 进程 vs 协程

| 特性 | 线程 | 进程 | 协程 |
|------|------|------|------|
| **创建开销** | 小（几 MB） | 大（几十 MB） | 极小（几 KB） |
| **切换开销** | 中等（内核态切换） | 大（上下文切换） | 极小（用户态切换） |
| **内存占用** | 共享进程内存 | 独立内存空间 | 共享线程内存 |
| **数据共享** | 直接共享（需同步） | IPC 机制（队列、管道等） | 直接共享 |
| **并行性** | 受 GIL 限制（单核） | 多核并行 | 单线程并发 |
| **隔离性** | 低（相互影响） | 高（完全隔离） | 中 |
| **适用场景** | I/O 密集型 | CPU 密集型 | I/O 密集型 |
| **通信成本** | 低（共享内存） | 高（IPC） | 极低（函数调用） |
| **稳定性** | 中（一个线程崩溃可能影响整个进程） | 高（进程隔离） | 高 |
| **Python 模块** | `threading` | `multiprocessing` | `asyncio` |

### 2.1 选择指南

1. **CPU 密集型任务**：使用多进程（绕过 GIL）
2. **I/O 密集型任务**：使用多线程或协程
3. **高并发网络服务**：使用协程（`asyncio`）
4. **需要隔离性的任务**：使用多进程
5. **简单并发需求**：使用多线程

## 3. 创建线程的多种方式

### 3.1 使用 `threading.Thread`

这是最常用的创建线程方式：

```python
import threading
import time
import os

def worker(name, delay):
    """工作线程函数"""
    print(f"Thread {name} started (PID: {os.getpid()}, TID: {threading.get_ident()})")
    time.sleep(delay)
    print(f"Thread {name} finished")
    return f"Result from {name}"

if __name__ == "__main__":
    # 创建线程
    thread1 = threading.Thread(target=worker, args=("A", 2))
    thread2 = threading.Thread(target=worker, args=("B", 1))
    
    # 启动线程
    thread1.start()
    thread2.start()
    
    # 等待线程结束
    thread1.join()
    thread2.join()
    
    print("All threads completed")
```

### 3.2 继承 `threading.Thread` 类

创建自定义线程类：

```python
import threading
import time

class WorkerThread(threading.Thread):
    def __init__(self, name, delay):
        super().__init__()
        self.name = name
        self.delay = delay
        self.result = None
    
    def run(self):
        """重写 run 方法"""
        print(f"Worker {self.name} started")
        time.sleep(self.delay)
        print(f"Worker {self.name} finished")
        self.result = f"Result from {self.name}"
    
    def get_result(self):
        """获取线程结果"""
        return self.result

if __name__ == "__main__":
    threads = []
    
    # 创建并启动多个线程
    for i in range(3):
        thread = WorkerThread(f"Worker-{i}", i+1)
        threads.append(thread)
        thread.start()
    
    # 等待所有线程完成并获取结果
    for thread in threads:
        thread.join()
        print(f"Result: {thread.get_result()}")
    
    print("All workers completed")
```

### 3.3 使用线程池

对于需要执行大量任务的情况，使用线程池更高效：

```python
import concurrent.futures
import time
import random

def task(n):
    """模拟任务"""
    worker_id = threading.get_ident()
    print(f"Task {n} started by thread {worker_id}")
    time.sleep(random.uniform(0.5, 1.5))
    result = n * n
    print(f"Task {n} finished with result {result}")
    return result

if __name__ == "__main__":
    # 使用 ThreadPoolExecutor
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        # 提交单个任务
        future = executor.submit(task, 10)
        print(f"Future result: {future.result()}")
        
        # 批量提交任务
        futures = [executor.submit(task, i) for i in range(5)]
        
        # 按完成顺序获取结果
        for future in concurrent.futures.as_completed(futures):
            print(f"Completed: {future.result()}")
        
        # 使用 map（按提交顺序返回结果）
        results = list(executor.map(task, range(1, 6)))
        print(f"Map results: {results}")
```

### 3.4 使用 `threading` 模块的旧式 API

Python 还提供了 `_thread` 模块（Python 2 中的 `thread`），但推荐使用 `threading`：

```python
import _thread
import time

def worker_thread(thread_name, delay):
    """使用 _thread 的工作线程"""
    print(f"Thread {thread_name} started")
    time.sleep(delay)
    print(f"Thread {thread_name} finished")

if __name__ == "__main__":
    # 启动线程
    _thread.start_new_thread(worker_thread, ("Thread-1", 2))
    _thread.start_new_thread(worker_thread, ("Thread-2", 1))
    
    # 等待线程完成（简单方式）
    time.sleep(3)
    print("Main thread exiting")
```

**注意**：`_thread` 模块功能有限，不提供高级同步机制，通常不推荐使用。

### 3.5 守护线程（Daemon Thread）

守护线程会在主线程退出时自动退出，无论是否完成任务：

```python
import threading
import time

def daemon_worker():
    """守护线程工作函数"""
    print("Daemon thread started")
    try:
        while True:
            print("Daemon thread working...")
            time.sleep(1)
    finally:
        print("Daemon thread exiting")

def normal_worker():
    """普通线程工作函数"""
    print("Normal thread started")
    time.sleep(3)
    print("Normal thread finished")

if __name__ == "__main__":
    # 创建守护线程
    daemon_thread = threading.Thread(target=daemon_worker)
    daemon_thread.daemon = True  # 设置为守护线程
    
    # 创建普通线程
    normal_thread = threading.Thread(target=normal_worker)
    
    # 启动线程
    daemon_thread.start()
    normal_thread.start()
    
    # 等待普通线程完成
    normal_thread.join()
    
    print("Main thread exiting (daemon thread will be terminated)")
    # 主线程退出时，守护线程会自动终止
```

## 4. 线程同步机制

由于线程共享内存，多个线程同时访问共享资源可能导致数据不一致。Python 提供了多种同步机制。

### 4.1 锁（Lock）

最基本的同步机制，确保同一时间只有一个线程访问共享资源：

```python
import threading
import time

class BankAccount:
    def __init__(self, balance=1000):
        self.balance = balance
        self.lock = threading.Lock()
    
    def deposit(self, amount):
        """存款"""
        with self.lock:  # 自动获取和释放锁
            old_balance = self.balance
            time.sleep(0.01)  # 模拟处理时间
            self.balance = old_balance + amount
            print(f"Deposited {amount}, new balance: {self.balance}")
    
    def withdraw(self, amount):
        """取款"""
        with self.lock:
            if self.balance >= amount:
                old_balance = self.balance
                time.sleep(0.01)
                self.balance = old_balance - amount
                print(f"Withdrew {amount}, new balance: {self.balance}")
                return True
            else:
                print(f"Insufficient funds for withdrawal of {amount}")
                return False

def customer_operations(account, operations, customer_id):
    """客户操作线程"""
    for op_type, amount in operations:
        if op_type == "deposit":
            account.deposit(amount)
        elif op_type == "withdraw":
            account.withdraw(amount)
        time.sleep(0.05)
    print(f"Customer {customer_id} completed")

if __name__ == "__main__":
    account = BankAccount(1000)
    
    # 定义客户操作
    customers = [
        [("deposit", 100), ("withdraw", 200), ("deposit", 50)],
        [("withdraw", 300), ("deposit", 150), ("withdraw", 100)],
        [("deposit", 200), ("withdraw", 400), ("deposit", 100)]
    ]
    
    threads = []
    for i, operations in enumerate(customers):
        thread = threading.Thread(
            target=customer_operations,
            args=(account, operations, i)
        )
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    print(f"Final balance: {account.balance}")
```

### 4.2 可重入锁（RLock）

允许同一个线程多次获取锁：

```python
import threading

class RecursiveCounter:
    def __init__(self):
        self.count = 0
        self.lock = threading.RLock()  # 可重入锁
    
    def increment(self, n=1):
        """递增计数器"""
        with self.lock:
            self.count += n
            if n > 1:
                # 递归调用自身，需要可重入锁
                self.increment(n - 1)
    
    def get_count(self):
        """获取计数值"""
        with self.lock:
            return self.count

def worker(counter, thread_id):
    """工作线程"""
    for i in range(3):
        counter.increment(3)  # 每次递增3，会递归调用
        print(f"Thread {thread_id}: count = {counter.get_count()}")
        threading.Event().wait(0.1)  # 短暂等待

if __name__ == "__main__":
    counter = RecursiveCounter()
    
    threads = []
    for i in range(3):
        thread = threading.Thread(target=worker, args=(counter, i))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    print(f"Final count: {counter.get_count()}")
```

### 4.3 信号量（Semaphore）

限制同时访问资源的线程数量：

```python
import threading
import time
import random

class ConnectionPool:
    def __init__(self, max_connections=3):
        self.semaphore = threading.Semaphore(max_connections)
        self.connections = []
    
    def get_connection(self, thread_id):
        """获取连接"""
        print(f"Thread {thread_id} waiting for connection...")
        self.semaphore.acquire()
        
        # 模拟获取连接
        connection_id = len(self.connections)
        self.connections.append(connection_id)
        print(f"Thread {thread_id} got connection {connection_id}")
        
        return connection_id
    
    def release_connection(self, thread_id, connection_id):
        """释放连接"""
        if connection_id in self.connections:
            self.connections.remove(connection_id)
            self.semaphore.release()
            print(f"Thread {thread_id} released connection {connection_id}")

def database_query(pool, thread_id, query_time):
    """数据库查询线程"""
    try:
        # 获取连接
        conn_id = pool.get_connection(thread_id)
        
        # 执行查询
        time.sleep(query_time)
        print(f"Thread {thread_id} executed query in {query_time:.2f}s")
        
        # 释放连接
        pool.release_connection(thread_id, conn_id)
    except Exception as e:
        print(f"Thread {thread_id} error: {e}")

if __name__ == "__main__":
    pool = ConnectionPool(max_connections=3)
    
    threads = []
    for i in range(10):
        query_time = random.uniform(0.5, 2.0)
        thread = threading.Thread(
            target=database_query,
            args=(pool, i, query_time)
        )
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    print("All queries completed")
```

### 4.4 事件（Event）

线程间通信机制，一个线程发出事件，其他线程等待事件：

```python
import threading
import time
import random

class DownloadManager:
    def __init__(self):
        self.data_ready = threading.Event()
        self.data = None
    
    def download_data(self):
        """下载数据线程"""
        print("Download thread: starting download...")
        time.sleep(2)  # 模拟下载时间
        
        # 下载完成
        self.data = {"size": 1024, "type": "json", "content": "sample data"}
        print("Download thread: data ready")
        
        # 设置事件，通知等待的线程
        self.data_ready.set()
    
    def process_data(self, processor_id):
        """处理数据线程"""
        print(f"Processor {processor_id}: waiting for data...")
        
        # 等待数据就绪
        self.data_ready.wait()
        
        # 处理数据
        print(f"Processor {processor_id}: processing data: {self.data}")
        time.sleep(random.uniform(0.5, 1.5))
        print(f"Processor {processor_id}: processing completed")

if __name__ == "__main__":
    manager = DownloadManager()
    
    # 创建下载线程
    download_thread = threading.Thread(target=manager.download_data)
    
    # 创建处理线程
    processors = []
    for i in range(3):
        processor = threading.Thread(target=manager.process_data, args=(i,))
        processors.append(processor)
    
    # 启动所有线程
    download_thread.start()
    for processor in processors:
        processor.start()
    
    # 等待线程完成
    download_thread.join()
    for processor in processors:
        processor.join()
    
    print("All operations completed")
```

### 4.5 条件变量（Condition）

更复杂的同步机制，用于线程间的等待/通知：

```python
import threading
import time
import random
import queue

class ProducerConsumer:
    def __init__(self, max_size=5):
        self.queue = queue.Queue(maxsize=max_size)
        self.condition = threading.Condition()
        self.producers_done = False
    
    def producer(self, producer_id):
        """生产者线程"""
        for i in range(5):
            item = f"Item-{producer_id}-{i}"
            time.sleep(random.uniform(0.1, 0.5))
            
            with self.condition:
                # 等待队列有空间
                while self.queue.full():
                    print(f"Producer {producer_id}: queue full, waiting...")
                    self.condition.wait()
                
                # 生产物品
                self.queue.put(item)
                print(f"Producer {producer_id}: produced {item}")
                
                # 通知消费者
                self.condition.notify_all()
        
        print(f"Producer {producer_id} finished")
    
    def consumer(self, consumer_id):
        """消费者线程"""
        while True:
            with self.condition:
                # 等待队列有物品
                while self.queue.empty():
                    if self.producers_done:
                        print(f"Consumer {consumer_id}: no more items, exiting")
                        return
                    print(f"Consumer {consumer_id}: queue empty, waiting...")
                    self.condition.wait()
                
                # 消费物品
                item = self.queue.get()
                print(f"Consumer {consumer_id}: consumed {item}")
                
                # 通知生产者
                self.condition.notify_all()
            
            # 模拟处理时间
            time.sleep(random.uniform(0.2, 0.8))
            self.queue.task_done()

if __name__ == "__main__":
    pc = ProducerConsumer(max_size=3)
    
    # 创建生产者
    producers = []
    for i in range(2):
        producer = threading.Thread(target=pc.producer, args=(i,))
        producers.append(producer)
        producer.start()
    
    # 创建消费者
    consumers = []
    for i in range(3):
        consumer = threading.Thread(target=pc.consumer, args=(i,))
        consumers.append(consumer)
        consumer.start()
    
    # 等待生产者完成
    for producer in producers:
        producer.join()
    
    # 通知消费者生产者已完成
    with pc.condition:
        pc.producers_done = True
        pc.condition.notify_all()
    
    # 等待消费者完成
    for consumer in consumers:
        consumer.join()
    
    print("All producers and consumers finished")
```

### 4.6 屏障（Barrier）

使多个线程在某个点同步，等待所有线程到达后才能继续：

```python
import threading
import time
import random

def worker(barrier, worker_id, phase_count):
    """工作线程，需要多个阶段同步"""
    for phase in range(phase_count):
        # 执行阶段工作
        work_time = random.uniform(0.5, 1.5)
        print(f"Worker {worker_id}: phase {phase} started (work: {work_time:.2f}s)")
        time.sleep(work_time)
        
        # 等待其他线程
        print(f"Worker {worker_id}: phase {phase} completed, waiting at barrier")
        barrier.wait()
        
        print(f"Worker {worker_id}: passed barrier for phase {phase}")
    
    print(f"Worker {worker_id}: all phases completed")

if __name__ == "__main__":
    # 创建屏障，需要4个线程到达
    worker_count = 4
    phase_count = 3
    barrier = threading.Barrier(worker_count)
    
    threads = []
    for i in range(worker_count):
        thread = threading.Thread(
            target=worker,
            args=(barrier, i, phase_count)
        )
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    print("All workers completed all phases")
```

### 4.7 定时器（Timer）

在指定时间后执行函数：

```python
import threading
import time

def delayed_task(task_name, delay):
    """延迟任务"""
    print(f"Task '{task_name}' will execute after {delay} seconds")
    time.sleep(delay)
    print(f"Task '{task_name}' executed at {time.time()}")

def timer_example():
    """定时器示例"""
    print(f"Main thread started at {time.time()}")
    
    # 创建定时器（3秒后执行）
    timer1 = threading.Timer(3.0, delayed_task, args=("Task-1", 0.5))
    timer1.start()
    
    # 创建另一个定时器（5秒后执行）
    timer2 = threading.Timer(5.0, delayed_task, args=("Task-2", 1.0))
    timer2.start()
    
    # 可以取消定时器
    timer3 = threading.Timer(2.0, delayed_task, args=("Task-3", 0.5))
    timer3.start()
    time.sleep(1)
    print("Cancelling timer3...")
    timer3.cancel()  # 取消定时器
    
    # 等待定时器完成
    timer1.join()
    timer2.join()
    
    print("Main thread exiting")

if __name__ == "__main__":
    timer_example()
```

## 5. 线程安全问题和解决方案

### 5.1 竞态条件（Race Condition）

多个线程同时访问共享资源导致的不确定结果：

```python
import threading
import time

class UnsafeCounter:
    """不安全的计数器"""
    def __init__(self):
        self.value = 0
    
    def increment(self):
        """递增（不安全）"""
        current = self.value
        time.sleep(0.0001)  # 增加竞态条件发生概率
        self.value = current + 1

class SafeCounter:
    """安全的计数器"""
    def __init__(self):
        self.value = 0
        self.lock = threading.Lock()
    
    def increment(self):
        """递增（安全）"""
        with self.lock:
            current = self.value
            time.sleep(0.0001)
            self.value = current + 1

def test_counter(counter, counter_name):
    """测试计数器"""
    threads = []
    
    # 创建100个线程，每个递增100次
    for _ in range(100):
        thread = threading.Thread(target=lambda: [counter.increment() for _ in range(100)])
        threads.append(thread)
        thread.start()
    
    # 等待所有线程
    for thread in threads:
        thread.join()
    
    expected = 100 * 100  # 100个线程 × 100次递增 = 10000
    actual = counter.value
    print(f"{counter_name}: expected={expected}, actual={actual}, correct={expected == actual}")

if __name__ == "__main__":
    print("Testing unsafe counter...")
    unsafe_counter = UnsafeCounter()
    test_counter(unsafe_counter, "UnsafeCounter")
    
    print("\nTesting safe counter...")
    safe_counter = SafeCounter()
    test_counter(safe_counter, "SafeCounter")
```

### 5.2 死锁（Deadlock）

两个或多个线程相互等待对方释放锁：

```python
import threading
import time

class DeadlockExample:
    def __init__(self):
        self.lock_a = threading.Lock()
        self.lock_b = threading.Lock()
    
    def process_ab(self, thread_id):
        """先获取lock_a，再获取lock_b"""
        print(f"Thread {thread_id}: waiting for lock_a...")
        with self.lock_a:
            print(f"Thread {thread_id}: acquired lock_a")
            time.sleep(0.1)  # 增加死锁概率
            
            print(f"Thread {thread_id}: waiting for lock_b...")
            with self.lock_b:
                print(f"Thread {thread_id}: acquired lock_b")
                # 处理工作
                time.sleep(0.1)
        
        print(f"Thread {thread_id}: released locks")
    
    def process_ba(self, thread_id):
        """先获取lock_b，再获取lock_a"""
        print(f"Thread {thread_id}: waiting for lock_b...")
        with self.lock_b:
            print(f"Thread {thread_id}: acquired lock_b")
            time.sleep(0.1)
            
            print(f"Thread {thread_id}: waiting for lock_a...")
            with self.lock_a:
                print(f"Thread {thread_id}: acquired lock_a")
                # 处理工作
                time.sleep(0.1)
        
        print(f"Thread {thread_id}: released locks")

def deadlock_solution():
    """死锁解决方案"""
    lock_a = threading.Lock()
    lock_b = threading.Lock()
    
    def process_with_timeout(thread_id, lock1, lock2, timeout=1.0):
        """使用超时避免死锁"""
        print(f"Thread {thread_id}: trying to acquire first lock...")
        
        # 尝试获取第一个锁，设置超时
        if lock1.acquire(timeout=timeout):
            try:
                print(f"Thread {thread_id}: acquired first lock")
                time.sleep(0.1)
                
                print(f"Thread {thread_id}: trying to acquire second lock...")
                # 尝试获取第二个锁，设置超时
                if lock2.acquire(timeout=timeout):
                    try:
                        print(f"Thread {thread_id}: acquired second lock")
                        # 处理工作
                        time.sleep(0.1)
                    finally:
                        lock2.release()
                        print(f"Thread {thread_id}: released second lock")
                else:
                    print(f"Thread {thread_id}: timeout acquiring second lock")
            finally:
                lock1.release()
                print(f"Thread {thread_id}: released first lock")
        else:
            print(f"Thread {thread_id}: timeout acquiring first lock")
    
    def process_with_order(thread_id):
        """使用固定的锁顺序避免死锁"""
        # 总是按相同顺序获取锁
        locks = sorted([lock_a, lock_b], key=id)
        
        print(f"Thread {thread_id}: acquiring locks in fixed order...")
        for lock in locks:
            lock.acquire()
            print(f"Thread {thread_id}: acquired lock {id(lock)}")
            time.sleep(0.05)
        
        # 处理工作
        time.sleep(0.1)
        
        # 释放锁（顺序不重要）
        for lock in locks:
            lock.release()
            print(f"Thread {thread_id}: released lock {id(lock)}")
    
    # 测试超时方案
    print("\n=== Testing timeout solution ===")
    threads = []
    for i in range(2):
        if i % 2 == 0:
            thread = threading.Thread(target=process_with_timeout, args=(i, lock_a, lock_b))
        else:
            thread = threading.Thread(target=process_with_timeout, args=(i, lock_b, lock_a))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    # 测试固定顺序方案
    print("\n=== Testing fixed order solution ===")
    threads = []
    for i in range(3):
        thread = threading.Thread(target=process_with_order, args=(i,))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()

if __name__ == "__main__":
    # 演示死锁
    print("=== Demonstrating deadlock ===")
    example = DeadlockExample()
    
    thread1 = threading.Thread(target=example.process_ab, args=(1,))
    thread2 = threading.Thread(target=example.process_ba, args=(2,))
    
    thread1.start()
    thread2.start()
    
    # 等待一段时间看是否死锁
    thread1.join(timeout=2)
    thread2.join(timeout=2)
    
    if thread1.is_alive() or thread2.is_alive():
        print("DEADLOCK DETECTED! Threads are stuck.")
        # 在实际应用中，应该实现死锁检测和恢复机制
    
    # 演示解决方案
    deadlock_solution()
```

### 5.3 活锁（Livelock）

线程不断改变状态以响应其他线程，但无法继续执行：

```python
import threading
import time
import random

class LivelockExample:
    def __init__(self):
        self.lock_a = threading.Lock()
        self.lock_b = threading.Lock()
        self.attempts = [0, 0]
    
    def worker(self, worker_id, other_worker_id):
        """可能产生活锁的工作线程"""
        max_attempts = 10
        
        for attempt in range(max_attempts):
            self.attempts[worker_id] += 1
            
            # 尝试获取锁A
            if self.lock_a.acquire(blocking=False):
                print(f"Worker {worker_id}: acquired lock_a (attempt {attempt})")
                time.sleep(0.05)
                
                # 尝试获取锁B
                if self.lock_b.acquire(blocking=False):
                    print(f"Worker {worker_id}: acquired lock_b")
                    # 成功获取两个锁
                    time.sleep(0.1)
                    self.lock_b.release()
                    self.lock_a.release()
                    print(f"Worker {worker_id}: completed work")
                    return True
                else:
                    # 释放锁A，让其他线程有机会
                    self.lock_a.release()
                    print(f"Worker {worker_id}: couldn't get lock_b, releasing lock_a")
                    time.sleep(random.uniform(0.01, 0.1))  # 随机等待
            else:
                print(f"Worker {worker_id}: couldn't get lock_a (attempt {attempt})")
                time.sleep(random.uniform(0.01, 0.1))
        
        print(f"Worker {worker_id}: failed after {max_attempts} attempts")
        return False

def livelock_solution():
    """活锁解决方案"""
    lock_a = threading.Lock()
    lock_b = threading.Lock()
    
    def worker_with_backoff(worker_id):
        """使用退避算法的工人线程"""
        max_attempts = 20
        base_delay = 0.01
        
        for attempt in range(max_attempts):
            # 使用退避算法
            delay = base_delay * (2 ** attempt) + random.uniform(0, 0.01)
            
            # 尝试获取两个锁
            got_a = lock_a.acquire(blocking=False)
            if got_a:
                got_b = lock_b.acquire(blocking=False)
                if got_b:
                    # 成功获取两个锁
                    print(f"Worker {worker_id}: acquired both locks after {attempt} attempts")
                    time.sleep(0.1)
                    lock_b.release()
                    lock_a.release()
                    return True
                else:
                    # 释放锁A
                    lock_a.release()
            
            # 等待一段时间再重试
            time.sleep(delay)
        
        print(f"Worker {worker_id}: failed after {max_attempts} attempts")
        return False
    
    # 测试
    threads = []
    for i in range(3):
        thread = threading.Thread(target=worker_with_backoff, args=(i,))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()

if __name__ == "__main__":
    # 演示活锁
    print("=== Demonstrating livelock ===")
    example = LivelockExample()
    
    threads = []
    for i in range(2):
        other_id = 1 if i == 0 else 0
        thread = threading.Thread(target=example.worker, args=(i, other_id))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    print(f"Total attempts: {example.attempts}")
    
    # 演示解决方案
    print("\n=== Testing livelock solution ===")
    livelock_solution()
```

### 5.4 饥饿（Starvation）

某些线程长时间无法获取所需资源：

```python
import threading
import time

class ResourceManager:
    def __init__(self):
        self.resource = "Shared Resource"
        self.lock = threading.Lock()
        self.access_count = {}
    
    def access_resource(self, thread_id, priority="normal"):
        """访问资源"""
        if thread_id not in self.access_count:
            self.access_count[thread_id] = 0
        
        # 模拟不同优先级的访问模式
        if priority == "high":
            # 高优先级线程频繁访问
            for _ in range(10):
                with self.lock:
                    self.access_count[thread_id] += 1
                    print(f"High-priority thread {thread_id} accessing resource "
                          f"(count: {self.access_count[thread_id]})")
                time.sleep(0.01)
        else:
            # 低优先级线程偶尔访问
            with self.lock:
                self.access_count[thread_id] += 1
                print(f"Normal thread {thread_id} accessing resource "
                      f"(count: {self.access_count[thread_id]})")
            time.sleep(0.1)

def starvation_solution():
    """饥饿解决方案：使用公平锁"""
    import queue
    
    class FairResourceManager:
        def __init__(self):
            self.resource = "Fair Shared Resource"
            self.lock = threading.Lock()
            self.access_queue = queue.Queue()
            self.access_count = {}
            self.request_lock = threading.Lock()
        
        def request_access(self, thread_id):
            """请求访问资源"""
            with self.request_lock:
                request_id = time.time()
                self.access_queue.put((request_id, thread_id))
                return request_id
        
        def access_resource_fairly(self, thread_id):
            """公平地访问资源"""
            if thread_id not in self.access_count:
                self.access_count[thread_id] = 0
            
            # 请求访问
            request_id = self.request_access(thread_id)
            
            # 等待轮到本线程
            while True:
                with self.request_lock:
                    if not self.access_queue.empty():
                        next_request_id, next_thread_id = self.access_queue.queue[0]
                        if next_thread_id == thread_id and next_request_id == request_id:
                            # 轮到本线程
                            self.access_queue.get()
                            break
                
                # 短暂等待
                time.sleep(0.001)
            
            # 访问资源
            with self.lock:
                self.access_count[thread_id] += 1
                print(f"Thread {thread_id} accessing resource fairly "
                      f"(count: {self.access_count[thread_id]})")
            
            time.sleep(0.05)
    
    # 测试公平资源管理器
    manager = FairResourceManager()
    
    def fair_worker(thread_id, priority):
        """公平的工作线程"""
        for _ in range(5):
            if priority == "high":
                manager.access_resource_fairly(thread_id)
                time.sleep(0.01)
            else:
                manager.access_resource_fairly(thread_id)
                time.sleep(0.1)
    
    # 创建线程
    threads = []
    for i in range(5):
        priority = "high" if i < 2 else "normal"
        thread = threading.Thread(target=fair_worker, args=(i, priority))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    print("\nAccess counts:")
    for thread_id, count in manager.access_count.items():
        print(f"  Thread {thread_id}: {count}")

if __name__ == "__main__":
    # 演示饥饿
    print("=== Demonstrating starvation ===")
    manager = ResourceManager()
    
    # 创建高优先级线程（频繁访问）
    high_priority_threads = []
    for i in range(2):
        thread = threading.Thread(
            target=manager.access_resource,
            args=(f"high-{i}", "high")
        )
        high_priority_threads.append(thread)
    
    # 创建普通优先级线程（偶尔访问）
    normal_threads = []
    for i in range(3):
        thread = threading.Thread(
            target=manager.access_resource,
            args=(f"normal-{i}", "normal")
        )
        normal_threads.append(thread)
    
    # 启动线程
    for thread in high_priority_threads + normal_threads:
        thread.start()
    
    # 运行一段时间
    time.sleep(2)
    
    print("\nAccess counts (showing potential starvation):")
    for thread_id, count in manager.access_count.items():
        print(f"  {thread_id}: {count}")
    
    # 演示解决方案
    print("\n=== Testing starvation solution ===")
    starvation_solution()
```

## 6. 线程间通信

### 6.1 使用队列（Queue）

线程安全的队列是最常用的线程间通信方式：

```python
import threading
import queue
import time
import random

def producer(q, producer_id, item_count):
    """生产者线程"""
    for i in range(item_count):
        item = f"Product-{producer_id}-{i}"
        production_time = random.uniform(0.1, 0.5)
        time.sleep(production_time)
        
        q.put(item)
        print(f"Producer {producer_id}: produced {item} "
              f"(queue size: {q.qsize()})")
    
    # 发送结束信号
    q.put(f"END-{producer_id}")
    print(f"Producer {producer_id}: finished")

def consumer(q, consumer_id):
    """消费者线程"""
    end_count = 0
    total_producers = 2  # 假设有2个生产者
    
    while True:
        try:
            item = q.get(timeout=2)
            
            if item.startswith("END-"):
                end_count += 1
                print(f"Consumer {consumer_id}: received end signal from {item}")
                if end_count >= total_producers:
                    print(f"Consumer {consumer_id}: all producers finished")
                    break
            else:
                # 处理产品
                processing_time = random.uniform(0.2, 0.8)
                time.sleep(processing_time)
                print(f"Consumer {consumer_id}: consumed {item} "
                      f"(processing: {processing_time:.2f}s)")
            
            q.task_done()
        except queue.Empty:
            print(f"Consumer {consumer_id}: queue empty, timeout")
            break
    
    print(f"Consumer {consumer_id}: finished")

if __name__ == "__main__":
    # 创建线程安全队列
    q = queue.Queue(maxsize=10)
    
    # 创建生产者
    producers = []
    for i in range(2):
        producer_thread = threading.Thread(
            target=producer,
            args=(q, i, 5)
        )
        producers.append(producer_thread)
        producer_thread.start()
    
    # 创建消费者
    consumers = []
    for i in range(3):
        consumer_thread = threading.Thread(
            target=consumer,
            args=(q, i)
        )
        consumers.append(consumer_thread)
        consumer_thread.start()
    
    # 等待生产者完成
    for producer_thread in producers:
        producer_thread.join()
    
    # 等待消费者处理所有项目
    q.join()
    
    # 通知消费者退出
    for consumer_thread in consumers:
        consumer_thread.join()
    
    print("All threads completed")
```

### 6.2 使用共享变量和条件变量

```python
import threading
import time
import random

class SharedBuffer:
    def __init__(self, capacity):
        self.buffer = []
        self.capacity = capacity
        self.lock = threading.Lock()
        self.not_empty = threading.Condition(self.lock)
        self.not_full = threading.Condition(self.lock)
        self.producers_done = False
    
    def put(self, item):
        """放入项目"""
        with self.not_full:
            # 等待缓冲区有空位
            while len(self.buffer) >= self.capacity:
                print(f"Producer waiting (buffer full: {len(self.buffer)}/{self.capacity})")
                self.not_full.wait()
            
            # 放入项目
            self.buffer.append(item)
            print(f"Produced: {item} (buffer: {len(self.buffer)}/{self.capacity})")
            
            # 通知消费者
            self.not_empty.notify()
    
    def get(self):
        """获取项目"""
        with self.not_empty:
            # 等待缓冲区有项目
            while len(self.buffer) == 0:
                if self.producers_done:
                    return None
                print("Consumer waiting (buffer empty)")
                self.not_empty.wait()
            
            # 获取项目
            item = self.buffer.pop(0)
            print(f"Consumed: {item} (buffer: {len(self.buffer)}/{self.capacity})")
            
            # 通知生产者
            self.not_full.notify()
            
            return item
    
    def set_producers_done(self):
        """设置生产者完成标志"""
        with self.lock:
            self.producers_done = True
            self.not_empty.notify_all()

def producer_worker(buffer, producer_id, item_count):
    """生产者工作线程"""
    for i in range(item_count):
        item = f"Item-{producer_id}-{i}"
        time.sleep(random.uniform(0.1, 0.3))
        buffer.put(item)
    
    print(f"Producer {producer_id}: finished")

def consumer_worker(buffer, consumer_id):
    """消费者工作线程"""
    while True:
        item = buffer.get()
        if item is None:
            break
        
        # 处理项目
        time.sleep(random.uniform(0.2, 0.5))
        print(f"Consumer {consumer_id}: processed {item}")

if __name__ == "__main__":
    buffer = SharedBuffer(capacity=3)
    
    # 创建生产者
    producers = []
    for i in range(2):
        producer = threading.Thread(
            target=producer_worker,
            args=(buffer, i, 5)
        )
        producers.append(producer)
        producer.start()
    
    # 创建消费者
    consumers = []
    for i in range(3):
        consumer = threading.Thread(
            target=consumer_worker,
            args=(buffer, i)
        )
        consumers.append(consumer)
        consumer.start()
    
    # 等待生产者完成
    for producer in producers:
        producer.join()
    
    # 通知消费者生产者已完成
    buffer.set_producers_done()
    
    # 等待消费者完成
    for consumer in consumers:
        consumer.join()
    
    print("All threads completed")
```

### 6.3 使用 `threading.local()` 实现线程局部存储

```python
import threading
import time
import random

# 创建线程局部存储
thread_local = threading.local()

def get_thread_data():
    """获取线程局部数据"""
    if not hasattr(thread_local, 'data'):
        # 每个线程第一次调用时初始化
        thread_id = threading.get_ident()
        thread_local.data = {
            'thread_id': thread_id,
            'counter': 0,
            'start_time': time.time(),
            'random_seed': random.random()
        }
        print(f"Thread {thread_id}: initialized thread-local data")
    
    return thread_local.data

def worker(worker_id):
    """工作线程"""
    # 获取线程局部数据
    data = get_thread_data()
    
    for i in range(3):
        # 更新线程局部数据
        data['counter'] += 1
        
        # 使用线程局部随机种子
        random.seed(data['random_seed'])
        random_value = random.random()
        data['random_seed'] = random_value
        
        # 执行工作
        work_time = random.uniform(0.1, 0.3)
        time.sleep(work_time)
        
        print(f"Worker {worker_id} (TID: {data['thread_id']}): "
              f"iteration {i}, counter={data['counter']}, "
              f"random={random_value:.3f}, work_time={work_time:.2f}s")
    
    elapsed = time.time() - data['start_time']
    print(f"Worker {worker_id} finished in {elapsed:.2f}s")

if __name__ == "__main__":
    threads = []
    
    # 创建并启动线程
    for i in range(4):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)
        thread.start()
    
    # 等待所有线程
    for thread in threads:
        thread.join()
    
    print("All workers completed")
```

## 7. 线程池高级用法

### 7.1 使用 `concurrent.futures.ThreadPoolExecutor`

```python
import concurrent.futures
import threading
import time
import random
import math

def cpu_intensive_task(n):
    """CPU 密集型任务（受 GIL 限制）"""
    result = 0
    for i in range(n * 10000):
        result += math.sin(i) * math.cos(i)
    return f"Task {n}: result={result:.2f}, thread={threading.get_ident()}"

def io_intensive_task(n):
    """I/O 密集型任务"""
    thread_id = threading.get_ident()
    print(f"Task {n} started in thread {thread_id}")
    
    # 模拟 I/O 等待
    wait_time = random.uniform(0.5, 1.5)
    time.sleep(wait_time)
    
    return f"Task {n}: waited {wait_time:.2f}s, thread={thread_id}"

def callback_example():
    """回调函数示例"""
    def task(n):
        time.sleep(random.uniform(0.1, 0.5))
        if random.random() < 0.1:
            raise ValueError(f"Task {n} failed randomly")
        return n * n
    
    def on_success(future):
        try:
            result = future.result()
            print(f"Task succeeded: {result}")
        except Exception as e:
            print(f"Task failed in callback: {e}")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for i in range(10):
            future = executor.submit(task, i)
            future.add_done_callback(on_success)
            futures.append(future)
        
        # 等待所有任务完成
        concurrent.futures.wait(futures)

def timeout_example():
    """超时处理示例"""
    def long_task(n):
        time.sleep(n)  # 睡眠 n 秒
        return f"Task {n} completed"
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        futures = [executor.submit(long_task, i) for i in range(1, 4)]
        
        try:
            # 等待所有任务完成，最多等待2秒
            results = []
            for future in concurrent.futures.as_completed(futures, timeout=2):
                try:
                    result = future.result(timeout=0.5)
                    results.append(result)
                except concurrent.futures.TimeoutError:
                    print(f"Task timed out: {future}")
            
            print(f"Completed results: {results}")
            
        except concurrent.futures.TimeoutError:
            print("Overall timeout occurred")
            
            # 取消未完成的任务
            for future in futures:
                if not future.done():
                    future.cancel()
                    print("Cancelled pending task")

def thread_pool_advanced():
    """线程池高级功能"""
    print("=== Testing CPU-intensive tasks ===")
    # CPU 密集型任务（受 GIL 影响）
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        cpu_futures = [executor.submit(cpu_intensive_task, i) for i in range(5)]
        
        for future in concurrent.futures.as_completed(cpu_futures):
            print(f"CPU result: {future.result()}")
    
    print("\n=== Testing I/O-intensive tasks ===")
    # I/O 密集型任务（适合多线程）
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        io_futures = [executor.submit(io_intensive_task, i) for i in range(5)]
        
        for future in concurrent.futures.as_completed(io_futures):
            print(f"I/O result: {future.result()}")
    
    print("\n=== Testing callbacks ===")
    callback_example()
    
    print("\n=== Testing timeouts ===")
    timeout_example()

if __name__ == "__main__":
    thread_pool_advanced()
```

### 7.2 自定义线程池

```python
import threading
import queue
import time
import random

class CustomThreadPool:
    """自定义线程池"""
    def __init__(self, num_workers):
        self.task_queue = queue.Queue()
        self.workers = []
        self.results = {}
        self.result_lock = threading.Lock()
        self.next_task_id = 0
        self.task_id_lock = threading.Lock()
        
        # 创建工作线程
        for i in range(num_workers):
            worker = threading.Thread(
                target=self._worker_loop,
                args=(i,),
                daemon=True
            )
            self.workers.append(worker)
            worker.start()
    
    def _worker_loop(self, worker_id):
        """工作线程循环"""
        print(f"Worker {worker_id} started")
        
        while True:
            try:
                # 获取任务
                task_id, func, args, kwargs = self.task_queue.get(timeout=1)
                
                try:
                    # 执行任务
                    print(f"Worker {worker_id}: executing task {task_id}")
                    result = func(*args, **kwargs)
                    
                    # 存储结果
                    with self.result_lock:
                        self.results[task_id] = {
                            'status': 'completed',
                            'result': result,
                            'worker_id': worker_id
                        }
                
                except Exception as e:
                    # 存储异常
                    with self.result_lock:
                        self.results[task_id] = {
                            'status': 'failed',
                            'error': str(e),
                            'worker_id': worker_id
                        }
                
                finally:
                    self.task_queue.task_done()
                    
            except queue.Empty:
                # 队列为空，继续等待
                continue
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
    
    def submit(self, func, *args, **kwargs):
        """提交任务"""
        with self.task_id_lock:
            task_id = self.next_task_id
            self.next_task_id += 1
        
        # 将任务加入队列
        self.task_queue.put((task_id, func, args, kwargs))
        
        return task_id
    
    def get_result(self, task_id, timeout=None):
        """获取任务结果"""
        start_time = time.time()
        
        while True:
            with self.result_lock:
                if task_id in self.results:
                    result_info = self.results[task_id]
                    
                    if result_info['status'] == 'completed':
                        return result_info['result']
                    else:
                        raise RuntimeError(f"Task failed: {result_info['error']}")
            
            # 检查超时
            if timeout is not None and (time.time() - start_time) > timeout:
                raise TimeoutError(f"Timeout waiting for task {task_id}")
            
            # 短暂等待
            time.sleep(0.01)
    
    def map(self, func, iterable):
        """批量执行任务"""
        task_ids = []
        
        # 提交所有任务
        for item in iterable:
            if isinstance(item, tuple):
                task_id = self.submit(func, *item)
            else:
                task_id = self.submit(func, item)
            task_ids.append(task_id)
        
        # 收集结果
        results = []
        for task_id in task_ids:
            try:
                result = self.get_result(task_id, timeout=10)
                results.append(result)
            except Exception as e:
                results.append(e)
        
        return results
    
    def shutdown(self, wait=True):
        """关闭线程池"""
        if wait:
            # 等待所有任务完成
            self.task_queue.join()
        
        # 清空队列
        while not self.task_queue.empty():
            try:
                self.task_queue.get_nowait()
                self.task_queue.task_done()
            except queue.Empty:
                break

def test_custom_thread_pool():
    """测试自定义线程池"""
    def task(n):
        """测试任务"""
        worker_id = threading.get_ident()
        work_time = random.uniform(0.1, 0.5)
        time.sleep(work_time)
        
        if random.random() < 0.1:
            raise ValueError(f"Task {n} failed in thread {worker_id}")
        
        return f"Task {n}: completed in {work_time:.2f}s by thread {worker_id}"
    
    # 创建线程池
    pool = CustomThreadPool(num_workers=3)
    
    # 提交任务
    task_ids = []
    for i in range(10):
        task_id = pool.submit(task, i)
        task_ids.append(task_id)
        print(f"Submitted task {i} with ID {task_id}")
    
    # 获取结果
    print("\nGetting results:")
    for task_id in task_ids:
        try:
            result = pool.get_result(task_id, timeout=5)
            print(f"  Task {task_id}: {result}")
        except Exception as e:
            print(f"  Task {task_id}: ERROR - {e}")
    
    # 测试 map 功能
    print("\nTesting map function:")
    numbers = list(range(5))
    results = pool.map(task, numbers)
    
    for i, result in enumerate(results):
        print(f"  Map result {i}: {result}")
    
    # 关闭线程池
    pool.shutdown()
    print("\nThread pool shutdown completed")

if __name__ == "__main__":
    test_custom_thread_pool()
```

### 7.3 动态调整线程池大小

```python
import threading
import time
import random
from concurrent.futures import ThreadPoolExecutor
import queue

class DynamicThreadPool:
    """动态调整大小的线程池"""
    def __init__(self, min_workers=1, max_workers=10):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.current_workers = min_workers
        
        self.task_queue = queue.Queue()
        self.workers = []
        self.worker_lock = threading.Lock()
        self.monitor_thread = None
        self.running = False
        
        # 初始工作线程
        self._adjust_workers(min_workers)
    
    def _worker_loop(self, worker_id):
        """工作线程循环"""
        print(f"Worker {worker_id} started (total workers: {len(self.workers)})")
        
        while self.running:
            try:
                # 获取任务
                task = self.task_queue.get(timeout=1)
                
                # 执行任务
                func, args, kwargs = task
                try:
                    result = func(*args, **kwargs)
                    print(f"Worker {worker_id}: task completed")
                except Exception as e:
                    print(f"Worker {worker_id}: task failed: {e}")
                
                self.task_queue.task_done()
                
            except queue.Empty:
                # 队列为空，继续等待
                continue
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
                break
        
        print(f"Worker {worker_id} stopped")
    
    def _adjust_workers(self, target_count):
        """调整工作线程数量"""
        with self.worker_lock:
            current_count = len(self.workers)
            
            if target_count > current_count:
                # 增加工作线程
                for i in range(current_count, target_count):
                    if len(self.workers) >= self.max_workers:
                        break
                    
                    worker_id = len(self.workers)
                    worker = threading.Thread(
                        target=self._worker_loop,
                        args=(worker_id,),
                        daemon=True
                    )
                    self.workers.append(worker)
                    worker.start()
                    
                    print(f"Added worker {worker_id} (total: {len(self.workers)})")
            
            elif target_count < current_count:
                # 减少工作线程（通过自然退出）
                # 实际实现中需要更复杂的机制
                pass
    
    def _monitor_loop(self):
        """监控循环，动态调整线程池大小"""
        print("Monitor thread started")
        
        while self.running:
            # 监控队列大小
            queue_size = self.task_queue.qsize()
            worker_count = len(self.workers)
            
            # 根据队列大小调整线程数
            if queue_size > worker_count * 3 and worker_count < self.max_workers:
                # 队列积压，增加线程
                new_count = min(worker_count + 1, self.max_workers)
                self._adjust_workers(new_count)
            
            elif queue_size < worker_count and worker_count > self.min_workers:
                # 队列空闲，减少线程
                # 简化实现：不减少线程
                pass
            
            # 等待一段时间
            time.sleep(2)
    
    def submit(self, func, *args, **kwargs):
        """提交任务"""
        if not self.running:
            raise RuntimeError("Thread pool not started")
        
        self.task_queue.put((func, args, kwargs))
        
        # 检查是否需要增加线程
        queue_size = self.task_queue.qsize()
        worker_count = len(self.workers)
        
        if queue_size > worker_count * 2 and worker_count < self.max_workers:
            self._adjust_workers(worker_count + 1)
    
    def start(self):
        """启动线程池"""
        if self.running:
            return
        
        self.running = True
        
        # 启动监控线程
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            daemon=True
        )
        self.monitor_thread.start()
        
        print(f"Dynamic thread pool started with {len(self.workers)} workers")
    
    def shutdown(self, wait=True):
        """关闭线程池"""
        self.running = False
        
        if wait:
            # 等待任务队列清空
            self.task_queue.join()
        
        # 等待工作线程结束
        with self.worker_lock:
            for worker in self.workers:
                if worker.is_alive():
                    worker.join(timeout=1)
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=1)
        
        print("Dynamic thread pool shutdown completed")

def test_dynamic_thread_pool():
    """测试动态线程池"""
    def task(task_id, duration):
        """测试任务"""
        thread_id = threading.get_ident()
        print(f"Task {task_id} started in thread {thread_id}")
        time.sleep(duration)
        return f"Task {task_id} completed in {duration:.2f}s"
    
    # 创建动态线程池
    pool = DynamicThreadPool(min_workers=2, max_workers=5)
    pool.start()
    
    # 提交任务（模拟突发负载）
    print("\n=== Phase 1: Normal load ===")
    for i in range(5):
        duration = random.uniform(0.5, 1.0)
        pool.submit(task, i, duration)
        time.sleep(0.2)
    
    time.sleep(2)
    
    print("\n=== Phase 2: High load ===")
    # 突发大量任务
    for i in range(10, 25):
        duration = random.uniform(0.1, 0.3)
        pool.submit(task, i, duration)
        time.sleep(0.05)  # 快速提交
    
    # 等待任务处理
    time.sleep(5)
    
    print("\n=== Phase 3: Low load ===")
    for i in range(30, 35):
        duration = random.uniform(1.0, 2.0)
        pool.submit(task, i, duration)
        time.sleep(0.5)
    
    # 等待并关闭
    time.sleep(5)
    pool.shutdown()

if __name__ == "__main__":
    test_dynamic_thread_pool()
```

## 8. 实际应用示例

### 8.1 并行下载文件

```python
import threading
import queue
import requests
import os
import time
from urllib.parse import urlparse

class ParallelDownloader:
    """并行下载器"""
    def __init__(self, max_workers=5, download_dir="downloads"):
        self.max_workers = max_workers
        self.download_dir = download_dir
        self.task_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.workers = []
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'bytes_downloaded': 0
        }
        self.stats_lock = threading.Lock()
        
        # 创建下载目录
        os.makedirs(download_dir, exist_ok=True)
    
    def _download_worker(self, worker_id):
        """下载工作线程"""
        print(f"Download worker {worker_id} started")
        
        while True:
            try:
                # 获取下载任务
                task_id, url, filename = self.task_queue.get(timeout=5)
                
                try:
                    # 执行下载
                    print(f"Worker {worker_id}: downloading {url}")
                    start_time = time.time()
                    
                    response = requests.get(url, stream=True, timeout=30)
                    response.raise_for_status()
                    
                    # 确定保存路径
                    if filename is None:
                        # 从 URL 提取文件名
                        parsed = urlparse(url)
                        filename = os.path.basename(parsed.path)
                        if not filename:
                            filename = f"download_{task_id}.bin"
                    
                    save_path = os.path.join(self.download_dir, filename)
                    
                    # 保存文件
                    total_size = 0
                    with open(save_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                total_size += len(chunk)
                    
                    elapsed = time.time() - start_time
                    speed = total_size / elapsed / 1024  # KB/s
                    
                    # 更新统计信息
                    with self.stats_lock:
                        self.stats['success'] += 1
                        self.stats['bytes_downloaded'] += total_size
                    
                    # 报告结果
                    self.result_queue.put({
                        'task_id': task_id,
                        'url': url,
                        'status': 'success',
                        'filename': save_path,
                        'size': total_size,
                        'time': elapsed,
                        'speed': speed
                    })
                    
                    print(f"Worker {worker_id}: downloaded {url} "
                          f"({total_size/1024:.1f} KB, {speed:.1f} KB/s)")
                    
                except Exception as e:
                    # 下载失败
                    with self.stats_lock:
                        self.stats['failed'] += 1
                    
                    self.result_queue.put({
                        'task_id': task_id,
                        'url': url,
                        'status': 'failed',
                        'error': str(e)
                    })
                    
                    print(f"Worker {worker_id}: failed to download {url}: {e}")
                
                finally:
                    self.task_queue.task_done()
                    
            except queue.Empty:
                # 队列为空，退出线程
                break
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
                break
        
        print(f"Download worker {worker_id} stopped")
    
    def add_download(self, url, filename=None):
        """添加下载任务"""
        task_id = self.stats['total']
        self.stats['total'] += 1
        self.task_queue.put((task_id, url, filename))
        return task_id
    
    def start(self):
        """启动下载器"""
        # 创建工作线程
        for i in range(self.max_workers):
            worker = threading.Thread(
                target=self._download_worker,
                args=(i,),
                daemon=True
            )
            self.workers.append(worker)
            worker.start()
        
        print(f"Downloader started with {self.max_workers} workers")
    
    def wait_completion(self):
        """等待所有任务完成"""
        self.task_queue.join()
        
        # 收集结果
        results = []
        while not self.result_queue.empty():
            results.append(self.result_queue.get())
        
        return results
    
    def get_stats(self):
        """获取统计信息"""
        with self.stats_lock:
            stats = self.stats.copy()
            stats['completion_rate'] = (stats['success'] / stats['total'] * 100 
                                        if stats['total'] > 0 else 0)
        return stats

def test_parallel_downloader():
    """测试并行下载器"""
    # 测试用的公开图片 URL（可以替换为实际需要下载的 URL）
    test_urls = [
        ("https://via.placeholder.com/150", "image1.jpg"),
        ("https://via.placeholder.com/200", "image2.jpg"),
        ("https://via.placeholder.com/250", "image3.jpg"),
        ("https://via.placeholder.com/300", "image4.jpg"),
        ("https://via.placeholder.com/350", "image5.jpg"),
    ]
    
    # 创建下载器
    downloader = ParallelDownloader(max_workers=3, download_dir="test_downloads")
    downloader.start()
    
    # 添加下载任务
    for url, filename in test_urls:
        downloader.add_download(url, filename)
    
    # 等待下载完成
    print("\nWaiting for downloads to complete...")
    results = downloader.wait_completion()
    
    # 显示结果
    print("\nDownload results:")
    for result in results:
        if result['status'] == 'success':
            print(f"  ✓ {result['url']}: {result['size']/1024:.1f} KB, "
                  f"{result['time']:.2f}s, {result['speed']:.1f} KB/s")
        else:
            print(f"  ✗ {result['url']}: {result['error']}")
    
    # 显示统计信息
    stats = downloader.get_stats()
    print(f"\nStatistics:")
    print(f"  Total tasks: {stats['total']}")
    print(f"  Successful: {stats['success']}")
    print(f"  Failed: {stats['failed']}")
    print(f"  Completion rate: {stats['completion_rate']:.1f}%")
    print(f"  Total bytes: {stats['bytes_downloaded']/1024:.1f} KB")

if __name__ == "__main__":
    # 注意：在实际运行前，请确保有网络连接
    # 并且 test_urls 中的 URL 是可访问的
    print("Note: This example requires internet connection")
    print("and the test URLs must be accessible.")
    # test_parallel_downloader()  # 取消注释以运行
```

### 8.2 并发 Web 服务器请求

```python
import threading
import queue
import requests
import time
import json
from concurrent.futures import ThreadPoolExecutor

class ConcurrentAPIRequester:
    """并发 API 请求器"""
    def __init__(self, max_workers=10, rate_limit=10):
        self.max_workers = max_workers
        self.rate_limit = rate_limit  # 每秒请求数
        self.semaphore = threading.Semaphore(rate_limit)
        self.last_request_time = time.time()
        self.request_lock = threading.Lock()
    
    def _rate_limited_request(self):
        """速率限制的请求"""
        with self.request_lock:
            current_time = time.time()
            elapsed = current_time - self.last_request_time
            min_interval = 1.0 / self.rate_limit
            
            if elapsed < min_interval:
                sleep_time = min_interval - elapsed
                time.sleep(sleep_time)
            
            self.last_request_time = time.time()
        
        # 获取信号量（限制并发数）
        self.semaphore.acquire()
        try:
            return True
        finally:
            # 延迟释放信号量，控制速率
            threading.Timer(1.0 / self.rate_limit, self.semaphore.release).start()
    
    def make_request(self, url, method='GET', params=None, data=None, headers=None):
        """发出请求"""
        # 应用速率限制
        self._rate_limited_request()
        
        try:
            if method.upper() == 'GET':
                response = requests.get(url, params=params, headers=headers, timeout=10)
            elif method.upper() == 'POST':
                response = requests.post(url, json=data, headers=headers, timeout=10)
            else:
                raise ValueError(f"Unsupported method: {method}")
            
            response.raise_for_status()
            
            # 尝试解析 JSON
            try:
                result = response.json()
            except:
                result = response.text
            
            return {
                'url': url,
                'status': 'success',
                'status_code': response.status_code,
                'data': result,
                'response_time': response.elapsed.total_seconds()
            }
            
        except requests.exceptions.RequestException as e:
            return {
                'url': url,
                'status': 'failed',
                'error': str(e),
                'response_time': None
            }
        except Exception as e:
            return {
                'url': url,
                'status': 'error',
                'error': str(e),
                'response_time': None
            }
    
    def concurrent_requests(self, requests_list):
        """并发执行多个请求"""
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # 提交所有请求
            futures = []
            for req in requests_list:
                future = executor.submit(
                    self.make_request,
                    req.get('url'),
                    req.get('method', 'GET'),
                    req.get('params'),
                    req.get('data'),
                    req.get('headers')
                )
                futures.append(future)
            
            # 收集结果
            for future in futures:
                try:
                    result = future.result(timeout=15)
                    results.append(result)
                except Exception as e:
                    results.append({
                        'status': 'timeout',
                        'error': str(e)
                    })
        
        return results
    
    def benchmark(self, url, num_requests=100):
        """性能基准测试"""
        print(f"Benchmarking {url} with {num_requests} requests...")
        
        start_time = time.time()
        
        # 创建请求列表
        requests_list = [{'url': url, 'method': 'GET'} for _ in range(num_requests)]
        
        # 执行并发请求
        results = self.concurrent_requests(requests_list)
        
        elapsed_time = time.time() - start_time
        
        # 分析结果
        success_count = sum(1 for r in results if r.get('status') == 'success')
        failed_count = num_requests - success_count
        
        # 计算响应时间
        response_times = [r.get('response_time', 0) 
                         for r in results if r.get('response_time')]
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0
        
        print(f"\nBenchmark results:")
        print(f"  Total requests: {num_requests}")
        print(f"  Successful: {success_count}")
        print(f"  Failed: {failed_count}")
        print(f"  Success rate: {success_count/num_requests*100:.1f}%")
        print(f"  Total time: {elapsed_time:.2f}s")
        print(f"  Requests per second: {num_requests/elapsed_time:.1f}")
        print(f"  Average response time: {avg_response_time*1000:.1f}ms")
        
        return {
            'total_requests': num_requests,
            'success_count': success_count,
            'failed_count': failed_count,
            'total_time': elapsed_time,
            'requests_per_second': num_requests / elapsed_time,
            'avg_response_time': avg_response_time
        }

def test_concurrent_requests():
    """测试并发请求"""
    # 创建请求器
    requester = ConcurrentAPIRequester(max_workers=5, rate_limit=20)
    
    # 测试单个请求
    print("=== Testing single request ===")
    result = requester.make_request('https://httpbin.org/get')
    print(f"Single request result: {json.dumps(result, indent=2)}")
    
    # 测试并发请求
    print("\n=== Testing concurrent requests ===")
    requests_list = [
        {'url': 'https://httpbin.org/get', 'params': {'page': i}} 
        for i in range(5)
    ]
    
    results = requester.concurrent_requests(requests_list)
    
    print(f"Concurrent requests completed: {len(results)}")
    for i, result in enumerate(results):
        status = result.get('status', 'unknown')
        print(f"  Request {i}: {status} "
              f"(code: {result.get('status_code', 'N/A')}, "
              f"time: {result.get('response_time', 0)*1000:.1f}ms)")
    
    # 性能基准测试（可选）
    # print("\n=== Running benchmark ===")
    # requester.benchmark('https://httpbin.org/delay/1', num_requests=10)

if __name__ == "__main__":
    test_concurrent_requests()
```

### 8.3 实时数据处理管道

```python
import threading
import queue
import time
import random
import json
from datetime import datetime

class DataProcessingPipeline:
    """实时数据处理管道"""
    def __init__(self):
        # 创建队列
        self.raw_data_queue = queue.Queue(maxsize=1000)
        self.processed_queue = queue.Queue(maxsize=1000)
        self.output_queue = queue.Queue(maxsize=1000)
        
        # 统计信息
        self.stats = {
            'raw_received': 0,
            'processed': 0,
            'output_sent': 0,
            'errors': 0
        }
        self.stats_lock = threading.Lock()
        
        # 控制标志
        self.running = False
        self.threads = []
    
    def data_generator(self, generator_id, interval=0.1):
        """数据生成器线程"""
        print(f"Data generator {generator_id} started")
        
        while self.running:
            try:
                # 生成模拟数据
                timestamp = datetime.now().isoformat()
                data = {
                    'generator_id': generator_id,
                    'timestamp': timestamp,
                    'value': random.uniform(0, 100),
                    'sensor_type': random.choice(['temperature', 'pressure', 'humidity']),
                    'quality': random.uniform(0.8, 1.0)
                }
                
                # 放入原始数据队列
                self.raw_data_queue.put(data, timeout=1)
                
                with self.stats_lock:
                    self.stats['raw_received'] += 1
                
                # 控制生成速率
                time.sleep(interval)
                
            except queue.Full:
                print(f"Generator {generator_id}: raw data queue full, waiting...")
                time.sleep(0.1)
            except Exception as e:
                print(f"Generator {generator_id} error: {e}")
                with self.stats_lock:
                    self.stats['errors'] += 1
    
    def data_processor(self, processor_id):
        """数据处理线程"""
        print(f"Data processor {processor_id} started")
        
        while self.running:
            try:
                # 获取原始数据
                raw_data = self.raw_data_queue.get(timeout=1)
                
                try:
                    # 处理数据
                    processed_data = self._process_data(raw_data, processor_id)
                    
                    # 放入处理队列
                    self.processed_queue.put(processed_data, timeout=1)
                    
                    with self.stats_lock:
                        self.stats['processed'] += 1
                        
                except Exception as e:
                    print(f"Processor {processor_id} processing error: {e}")
                    with self.stats_lock:
                        self.stats['errors'] += 1
                
                finally:
                    self.raw_data_queue.task_done()
                    
            except queue.Empty:
                # 队列为空，继续等待
                continue
            except Exception as e:
                print(f"Processor {processor_id} error: {e}")
    
    def _process_data(self, raw_data, processor_id):
        """处理数据的具体逻辑"""
        # 模拟处理时间
        time.sleep(random.uniform(0.01, 0.05))
        
        # 数据处理逻辑
        processed = raw_data.copy()
        processed['processor_id'] = processor_id
        processed['processed_at'] = datetime.now().isoformat()
        
        # 数据增强
        value = processed['value']
        if processed['sensor_type'] == 'temperature':
            processed['value_c'] = value
            processed['value_f'] = value * 9/5 + 32
        elif processed['sensor_type'] == 'pressure':
            processed['value_kpa'] = value
            processed['value_bar'] = value / 100
        elif processed['sensor_type'] == 'humidity':
            processed['value_percent'] = value
        
        # 质量检查
        if processed['quality'] < 0.9:
            processed['quality_flag'] = 'WARNING'
        else:
            processed['quality_flag'] = 'OK'
        
        return processed
    
    def output_writer(self, writer_id, output_file=None):
        """输出写入线程"""
        print(f"Output writer {writer_id} started")
        
        # 打开输出文件
        if output_file:
            file = open(output_file, 'a')
        else:
            file = None
        
        try:
            while self.running:
                try:
                    # 获取处理后的数据
                    processed_data = self.processed_queue.get(timeout=1)
                    
                    try:
                        # 写入输出
                        output = self._write_output(processed_data, writer_id, file)
                        
                        # 放入输出队列（用于其他用途）
                        self.output_queue.put(output, timeout=1)
                        
                        with self.stats_lock:
                            self.stats['output_sent'] += 1
                            
                    except Exception as e:
                        print(f"Writer {writer_id} output error: {e}")
                        with self.stats_lock:
                            self.stats['errors'] += 1
                    
                    finally:
                        self.processed_queue.task_done()
                        
                except queue.Empty:
                    # 队列为空，继续等待
                    continue
                except Exception as e:
                    print(f"Writer {writer_id} error: {e}")
        
        finally:
            # 关闭文件
            if file:
                file.close()
    
    def _write_output(self, data, writer_id, file):
        """写入输出的具体逻辑"""
        # 模拟写入时间
        time.sleep(random.uniform(0.005, 0.02))
        
        # 添加写入信息
        output = data.copy()
        output['writer_id'] = writer_id
        output['written_at'] = datetime.now().isoformat()
        
        # 写入文件（如果提供）
        if file:
            file.write(json.dumps(output) + '\n')
            file.flush()
        
        return output
    
    def monitor(self, interval=2):
        """监控线程"""
        print("Monitor thread started")
        
        while self.running:
            time.sleep(interval)
            
            with self.stats_lock:
                stats = self.stats.copy()
            
            print(f"\n=== Pipeline Statistics ===")
            print(f"Raw data received: {stats['raw_received']}")
            print(f"Data processed: {stats['processed']}")
            print(f"Output sent: {stats['output_sent']}")
            print(f"Errors: {stats['errors']}")
            
            # 队列状态
            print(f"Queue sizes: raw={self.raw_data_queue.qsize()}, "
                  f"processed={self.processed_queue.qsize()}, "
                  f"output={self.output_queue.qsize()}")
            
            # 计算处理速率
            if stats['processed'] > 0:
                rate = stats['processed'] / (interval * 10)  # 每10秒的速率
                print(f"Processing rate: {rate:.1f} items/sec")
    
    def start(self, num_generators=2, num_processors=3, num_writers=2):
        """启动管道"""
        self.running = True
        
        # 启动数据生成器
        for i in range(num_generators):
            thread = threading.Thread(
                target=self.data_generator,
                args=(i, random.uniform(0.05, 0.2)),
                daemon=True
            )
            self.threads.append(thread)
            thread.start()
        
        # 启动数据处理器
        for i in range(num_processors):
            thread = threading.Thread(
                target=self.data_processor,
                args=(i,),
                daemon=True
            )
            self.threads.append(thread)
            thread.start()
        
        # 启动输出写入器
        for i in range(num_writers):
            output_file = f"output_{i}.jsonl" if i < 2 else None
            thread = threading.Thread(
                target=self.output_writer,
                args=(i, output_file),
                daemon=True
            )
            self.threads.append(thread)
            thread.start()
        
        # 启动监控器
        monitor_thread = threading.Thread(
            target=self.monitor,
            daemon=True
        )
        self.threads.append(monitor_thread)
        monitor_thread.start()
        
        print(f"Pipeline started with {num_generators} generators, "
              f"{num_processors} processors, {num_writers} writers")
    
    def stop(self):
        """停止管道"""
        print("\nStopping pipeline...")
        self.running = False
        
        # 等待队列清空
        self.raw_data_queue.join()
        self.processed_queue.join()
        
        # 等待线程结束
        for thread in self.threads:
            if thread.is_alive():
                thread.join(timeout=2)
        
        print("Pipeline stopped")
        
        # 打印最终统计
        print("\n=== Final Statistics ===")
        for key, value in self.stats.items():
            print(f"{key}: {value}")

def test_data_pipeline():
    """测试数据处理管道"""
    pipeline = DataProcessingPipeline()
    
    try:
        # 启动管道
        pipeline.start(
            num_generators=2,
            num_processors=4,
            num_writers=2
        )
        
        # 运行一段时间
        print("\nPipeline running for 10 seconds...")
        time.sleep(10)
        
    except KeyboardInterrupt:
        print("\nInterrupted by user")
    finally:
        # 停止管道
        pipeline.stop()

if __name__ == "__main__":
    test_data_pipeline()
```

## 9. 高级主题

### 9.1 GIL 的工作原理和影响

```python
import threading
import time
import multiprocessing

def gil_demonstration():
    """演示 GIL 对 CPU 密集型任务的影响"""
    
    def cpu_intensive_work(iterations):
        """CPU 密集型工作"""
        result = 0
        for i in range(iterations):
            result += i * i
        return result
    
    def io_intensive_work(iterations):
        """I/O 密集型工作"""
        for i in range(iterations):
            time.sleep(0.001)  # 模拟 I/O 等待
        return iterations
    
    def test_cpu_intensive():
        """测试 CPU 密集型任务"""
        iterations = 10_000_000
        
        print("=== CPU-intensive task (受 GIL 影响) ===")
        
        # 单线程
        start = time.time()
        cpu_intensive_work(iterations)
        single_time = time.time() - start
        print(f"Single thread: {single_time:.2f}s")
        
        # 多线程（由于 GIL，不会更快）
        start = time.time()
        threads = []
        for _ in range(4):
            thread = threading.Thread(target=cpu_intensive_work, args=(iterations//4,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        multi_time = time.time() - start
        print(f"4 threads: {multi_time:.2f}s")
        print(f"Speedup: {single_time/multi_time:.2f}x")
    
    def test_io_intensive():
        """测试 I/O 密集型任务"""
        iterations = 1000
        
        print("\n=== I/O-intensive task (不受 GIL 影响) ===")
        
        # 单线程
        start = time.time()
        io_intensive_work(iterations)
        single_time = time.time() - start
        print(f"Single thread: {single_time:.2f}s")
        
        # 多线程
        start = time.time()
        threads = []
        for _ in range(4):
            thread = threading.Thread(target=io_intensive_work, args=(iterations//4,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        multi_time = time.time() - start
        print(f"4 threads: {multi_time:.2f}s")
        print(f"Speedup: {single_time/multi_time:.2f}x")
    
    def bypass_gil_with_multiprocessing():
        """使用多进程绕过 GIL"""
        print("\n=== Bypassing GIL with multiprocessing ===")
        
        def cpu_worker(iterations):
            result = 0
            for i in range(iterations):
                result += i * i
            return result
        
        iterations = 10_000_000
        
        # 单进程
        start = time.time()
        cpu_worker(iterations)
        single_time = time.time() - start
        print(f"Single process: {single_time:.2f}s")
        
        # 多进程
        start = time.time()
        with multiprocessing.Pool(4) as pool:
            pool.map(cpu_worker, [iterations//4] * 4)
        
        multi_time = time.time() - start
        print(f"4 processes: {multi_time:.2f}s")
        print(f"Speedup: {single_time/multi_time:.2f}x")
    
    # 运行测试
    test_cpu_intensive()
    test_io_intensive()
    bypass_gil_with_multiprocessing()

if __name__ == "__main__":
    gil_demonstration()
```

### 9.2 线程与异步编程结合

```python
import threading
import asyncio
import time
import concurrent.futures

def threading_with_asyncio():
    """线程与异步编程结合"""
    
    # 同步阻塞函数
    def blocking_io():
        """模拟阻塞 I/O 操作"""
        print(f"Blocking I/O started in thread {threading.get_ident()}")
        time.sleep(2)
        return f"I/O result from thread {threading.get_ident()}"
    
    # 异步函数
    async def async_task(task_id):
        """异步任务"""
        print(f"Async task {task_id} started")
        await asyncio.sleep(1)
        return f"Async result {task_id}"
    
    async def run_in_thread_pool():
        """在线程池中运行阻塞函数"""
        loop = asyncio.get_running_loop()
        
        # 在线程池中运行阻塞函数
        print("\n1. Running blocking I/O in thread pool...")
        result = await loop.run_in_executor(None, blocking_io)
        print(f"Result: {result}")
    
    async def concurrent_async_tasks():
        """并发执行异步任务"""
        print("\n2. Running concurrent async tasks...")
        
        # 创建异步任务
        tasks = [async_task(i) for i in range(3)]
        
        # 并发执行
        results = await asyncio.gather(*tasks)
        
        for result in results:
            print(f"  {result}")
    
    async def mixed_execution():
        """混合执行：异步任务 + 线程池"""
        print("\n3. Mixed execution: async + thread pool...")
        
        # 创建事件循环
        loop = asyncio.get_event_loop()
        
        # 同时执行异步任务和线程池任务
        async_task1 = async_task(100)
        thread_task = loop.run_in_executor(None, blocking_io)
        async_task2 = async_task(200)
        
        # 等待所有任务完成
        results = await asyncio.gather(async_task1, thread_task, async_task2)
        
        for result in results:
            print(f"  {result}")
    
    async def thread_pool_executor():
        """使用 ThreadPoolExecutor"""
        print("\n4. Using ThreadPoolExecutor with asyncio...")
        
        # 创建线程池执行器
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as pool:
            loop = asyncio.get_running_loop()
            
            # 提交多个任务到线程池
            futures = []
            for i in range(5):
                future = loop.run_in_executor(pool, blocking_io)
                futures.append(future)
            
            # 等待所有任务完成
            results = await asyncio.gather(*futures)
            
            for result in results:
                print(f"  {result}")
    
    async def main():
        """主异步函数"""
        print("Starting threading + asyncio demonstration...")
        print(f"Main thread: {threading.get_ident()}")
        
        await run_in_thread_pool()
        await concurrent_async_tasks()
        await mixed_execution()
        await thread_pool_executor()
        
        print("\nAll tasks completed!")
    
    # 运行异步主函数
    asyncio.run(main())

if __name__ == "__main__":
    threading_with_asyncio()
```

### 9.3 线程安全和原子操作

```python
import threading
import time
import sys

def atomic_operations():
    """原子操作示例"""
    
    # Python 中的原子操作
    print("=== Atomic operations in Python ===")
    
    # 在 Python 中，以下操作是原子的（对于单个变量）：
    # 1. 读取或替换一个变量
    # 2. 读取或替换一个对象的属性
    # 3. 读取或替换一个列表或字典中的项
    # 4. 大多数内置类型（int, float, str, tuple）的操作
    
    # 但是，以下操作不是原子的：
    # 1. 多个操作（如 i += 1）
    # 2. 调用方法（如 list.append()）
    # 3. 用户自定义类的操作
    
    class AtomicCounter:
        """使用 threading.Lock 的原子计数器"""
        def __init__(self):
            self._value = 0
            self._lock = threading.Lock()
        
        def increment(self):
            """原子递增"""
            with self._lock:
                self._value += 1
        
        def get_value(self):
            """获取当前值"""
            with self._lock:
                return self._value
    
    class NonAtomicCounter:
        """非原子计数器"""
        def __init__(self):
            self._value = 0
        
        def increment(self):
            """非原子递增（不安全）"""
            # 这个操作不是原子的：
            # 1. 读取 self._value
            # 2. 计算 self._value + 1
            # 3. 写入 self._value
            self._value += 1
        
        def get_value(self):
            """获取当前值"""
            return self._value
    
    def test_counter(counter_class, num_threads=10, increments_per_thread=1000):
        """测试计数器"""
        counter = counter_class()
        
        def worker():
            for _ in range(increments_per_thread):
                counter.increment()
        
        threads = []
        for _ in range(num_threads):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        expected = num_threads * increments_per_thread
        actual = counter.get_value()
        
        return expected, actual
    
    print("\nTesting atomic counter:")
    expected, actual = test_counter(AtomicCounter, 10, 1000)
    print(f"Expected: {expected}, Actual: {actual}, Correct: {expected == actual}")
    
    print("\nTesting non-atomic counter (may show race condition):")
    expected, actual = test_counter(NonAtomicCounter, 10, 1000)
    print(f"Expected: {expected}, Actual: {actual}, Correct: {expected == actual}")
    
    # 使用原子类型
    print("\n=== Using atomic types from queue ===")
    
    def queue_based_counter():
        """使用 queue.Queue 实现的计数器"""
        import queue
        
        class QueueCounter:
            def __init__(self):
                self._queue = queue.Queue()
                self._queue.put(0)  # 初始值
            
            def increment(self):
                value = self._queue.get()
                value += 1
                self._queue.put(value)
                self._queue.task_done()
            
            def get_value(self):
                # 注意：这个方法不是线程安全的
                # 仅用于演示
                if not self._queue.empty():
                    value = self._queue.get()
                    self._queue.put(value)
                    return value
                return 0
        
        return QueueCounter
    
    print("\nTesting queue-based counter:")
    expected, actual = test_counter(queue_based_counter(), 10, 1000)
    print(f"Expected: {expected}, Actual: {actual}, Correct: {expected == actual}")

if __name__ == "__main__":
    atomic_operations()
```

### 9.4 线程调试和性能分析

```python
import threading
import time
import cProfile
import pstats
import io
import traceback

def thread_debugging():
    """线程调试技术"""
    
    # 1. 基本的线程信息
    print("=== Basic thread information ===")
    
    def worker(worker_id):
        """工作线程"""
        print(f"Worker {worker_id}: thread id = {threading.get_ident()}")
        print(f"Worker {worker_id}: native id = {threading.current_thread().native_id}")
        print(f"Worker {worker_id}: is alive = {threading.current_thread().is_alive()}")
        print(f"Worker {worker_id}: is daemon = {threading.current_thread().daemon}")
        time.sleep(1)
    
    threads = []
    for i in range(3):
        thread = threading.Thread(target=worker, args=(i,))
        threads.append(thread)
        thread.start()
    
    # 获取所有活动线程
    print(f"\nActive threads: {threading.active_count()}")
    for thread in threading.enumerate():
        print(f"  {thread.name} (id={thread.ident}, alive={thread.is_alive()})")
    
    # 等待线程完成
    for thread in threads:
        thread.join()
    
    # 2. 线程异常处理
    print("\n=== Thread exception handling ===")
    
    def worker_with_exception(worker_id):
        """可能抛出异常的工作线程"""
        try:
            if worker_id == 1:
                raise ValueError(f"Worker {worker_id}故意抛出异常")
            print(f"Worker {worker_id} completed successfully")
        except Exception as e:
            print(f"Worker {worker_id} caught exception: {e}")
            # 记录完整的堆栈跟踪
            traceback.print_exc()
    
    threads = []
    for i in range(3):
        thread = threading.Thread(target=worker_with_exception, args=(i,))
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()
    
    # 3. 使用 threading.excepthook 处理未捕获异常
    print("\n=== Using threading.excepthook ===")
    
    def custom_excepthook(args):
        """自定义异常处理钩子"""
        print(f"\nUncaught exception in thread {args.thread.name}:")
        print(f"  Exception: {args.exc_type.__name__}: {args.exc_value}")
        print(f"  Traceback:")
        traceback.print_tb(args.exc_traceback)
    
    # 设置全局异常处理钩子
    threading.excepthook = custom_excepthook
    
    def worker_that_crashes(worker_id):
        """会崩溃的工作线程"""
        if worker_id == 0:
            raise RuntimeError(f"Worker {worker_id}崩溃了！")
        print(f"Worker {worker_id}正常完成")
    
    threads = []
    for i in range(2):
        thread = threading.Thread(
            target=worker_that_crashes,
            args=(i,),
            name=f"WorkerThread-{i}"
        )
        threads.append(thread)
        thread.start()
    
    for thread in threads:
        thread.join()

def thread_profiling():
    """线程性能分析"""
    
    print("\n=== Thread profiling ===")
    
    def cpu_intensive_work(n):
        """CPU 密集型工作"""
        result = 0
        for i in range(n):
            result += i * i
        return result
    
    def io_intensive_work(n):
        """I/O 密集型工作"""
        for i in range(n):
            time.sleep(0.001)
        return n
    
    def profile_threads():
        """分析多线程性能"""
        
        # 创建性能分析器
        profiler = cProfile.Profile()
        profiler.enable()
        
        # 执行多线程任务
        threads = []
        
        # CPU 密集型线程
        cpu_thread = threading.Thread(
            target=lambda: cpu_intensive_work(1000000)
        )
        threads.append(cpu_thread)
        
        # I/O 密集型线程
        io_thread = threading.Thread(
            target=lambda: io_intensive_work(100)
        )
        threads.append(io_thread)
        
        # 启动线程
        for thread in threads:
            thread.start()
        
        # 等待线程完成
        for thread in threads:
            thread.join()
        
        # 停止性能分析
        profiler.disable()
        
        # 输出分析结果
        s = io.StringIO()
        ps = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        ps.print_stats()
        
        print("Profiling results:")
        print(s.getvalue()[:1000])  # 只打印前1000个字符
    
    # 运行性能分析
    profile_threads()
    
    # 4. 线程性能监控
    print("\n=== Thread performance monitoring ===")
    
    class MonitoredWorker:
        """可监控的工作线程"""
        def __init__(self, worker_id):
            self.worker_id = worker_id
            self.start_time = None
            self.end_time = None
            self.iteration_count = 0
            self.lock = threading.Lock()
        
        def work(self):
            """工作函数"""
            self.start_time = time.time()
            
            try:
                for i in range(10):
                    # 模拟工作
                    time.sleep(0.1)
                    
                    # 更新迭代计数
                    with self.lock:
                        self.iteration_count += 1
                
                self.end_time = time.time()
                
            except Exception as e:
                print(f"Worker {self.worker_id} error: {e}")
        
        def get_stats(self):
            """获取统计信息"""
            with self.lock:
                stats = {
                    'worker_id': self.worker_id,
                    'iterations': self.iteration_count,
                    'running': self.end_time is None
                }
                
                if self.start_time:
                    elapsed = (self.end_time or time.time()) - self.start_time
                    stats['elapsed'] = elapsed
                    if self.iteration_count > 0:
                        stats['iterations_per_second'] = self.iteration_count / elapsed
                
                return stats
    
    def monitor_workers(workers, interval=0.5):
        """监控工作线程"""
        print("Starting monitoring...")
        
        while any(w.get_stats()['running'] for w in workers):
            time.sleep(interval)
            
            print(f"\nMonitoring at {time.strftime('%H:%M:%S')}:")
            for worker in workers:
                stats = worker.get_stats()
                status = "RUNNING" if stats['running'] else "COMPLETED"
                print(f"  Worker {worker.worker_id}: {status}, "
                      f"iterations={stats['iterations']}, "
                      f"elapsed={stats.get('elapsed', 0):.1f}s")
    
    # 创建和启动工作线程
    workers = [MonitoredWorker(i) for i in range(3)]
    threads = []
    
    for worker in workers:
        thread = threading.Thread(target=worker.work)
        threads.append(thread)
        thread.start()
    
    # 启动监控线程
    monitor_thread = threading.Thread(
        target=monitor_workers,
        args=(workers,)
    )
    monitor_thread.start()
    
    # 等待工作线程完成
    for thread in threads:
        thread.join()
    
    # 等待监控线程完成
    monitor_thread.join()
    
    print("\nFinal statistics:")
    for worker in workers:
        stats = worker.get_stats()
        print(f"  Worker {worker.worker_id}: "
              f"{stats['iterations']} iterations in {stats['elapsed']:.2f}s "
              f"({stats.get('iterations_per_second', 0):.1f} iter/s)")

if __name__ == "__main__":
    thread_debugging()
    thread_profiling()
```

## 10. 最佳实践和注意事项

### 10.1 Python 线程编程最佳实践

1. **理解 GIL 的限制**
   - CPU 密集型任务：使用多进程
   - I/O 密集型任务：使用多线程或协程
   - 混合任务：考虑线程与进程结合

2. **合理设置线程数量**
   ```python
   # 根据任务类型设置线程数
   import os
   
   # I/O 密集型：可以设置较多线程
   io_workers = min(32, (os.cpu_count() or 1) + 4)
   
   # CPU 密集型：由于 GIL，增加线程数可能无益
   cpu_workers = os.cpu_count() or 1
   ```

3. **使用线程安全的数据结构**
   ```python
   import queue
   from threading import Lock
   
   # 使用 queue.Queue 进行线程间通信
   task_queue = queue.Queue()
   
   # 使用锁保护共享资源
   shared_data = []
   data_lock = Lock()
   
   with data_lock:
       shared_data.append("new item")
   ```

4. **避免死锁**
   - 总是以相同的顺序获取锁
   - 使用超时机制
   - 使用上下文管理器（`with lock:`）

5. **正确处理线程异常**
   ```python
   import threading
   import traceback
   
   def safe_worker():
       try:
           # 工作代码
           pass
       except Exception as e:
           print(f"Thread error: {e}")
           traceback.print_exc()
   
   # 设置全局异常处理
   def global_exception_handler(args):
       print(f"Uncaught exception in {args.thread.name}: {args.exc_value}")
   
   threading.excepthook = global_exception_handler
   ```

6. **使用线程池而非手动管理线程**
   ```python
   # 使用 concurrent.futures
   from concurrent.futures import ThreadPoolExecutor
   
   with ThreadPoolExecutor(max_workers=10) as executor:
       futures = [executor.submit(task, arg) for arg in args]
       results = [f.result() for f in futures]
   ```

### 10.2 常见陷阱和解决方案

1. **竞态条件**
   ```python
   # 错误示例
   counter = 0
   
   def unsafe_increment():
       global counter
       counter += 1  # 不是原子操作！
   
   # 正确示例
   import threading
   
   counter = 0
   counter_lock = threading.Lock()
   
   def safe_increment():
       global counter
       with counter_lock:
           counter += 1
   ```

2. **死锁**
   ```python
   # 错误示例：可能死锁
   lock1 = threading.Lock()
   lock2 = threading.Lock()
   
   def thread1():
       with lock1:
           with lock2:  # 如果 thread2 先获取了 lock2，就会死锁
               pass
   
   # 正确示例：固定锁顺序
   def acquire_locks(lock1, lock2):
       # 总是按相同顺序获取锁
       locks = sorted([lock1, lock2], key=id)
       for lock in locks:
           lock.acquire()
       return locks
   ```

3. **资源泄漏**
   ```python
   # 错误示例：线程可能永远不会结束
   def leaking_thread():
       while True:
           # 没有退出条件
           pass
   
   # 正确示例：提供退出机制
   import threading
   
   stop_event = threading.Event()
   
   def proper_thread():
       while not stop_event.is_set():
           # 定期检查停止标志
           pass
   ```

4. **线程间通信问题**
   ```python
   # 错误示例：直接使用共享变量
   shared_list = []
   
   def unsafe_add(item):
       shared_list.append(item)  # 可能丢失数据
   
   # 正确示例：使用队列
   import queue
   
   shared_queue = queue.Queue()
   
   def safe_add(item):
       shared_queue.put(item)
   ```

### 10.3 性能优化建议

1. **减少锁竞争**
   ```python
   # 不好：全局锁
   global_lock = threading.Lock()
   global_counter = 0
   
   # 较好：细粒度锁
   counters = [0] * 10
   locks = [threading.Lock() for _ in range(10)]
   
   def increment_counter(thread_id):
       index = thread_id % 10
       with locks[index]:
           counters[index] += 1
   ```

2. **使用本地变量**
   ```python
   # 不好：频繁访问共享变量
   def slow_worker():
       for i in range(1000):
           with shared_lock:
               shared_list.append(i)
   
   # 较好：批量处理
   def fast_worker():
       local_results = []
       for i in range(1000):
           local_results.append(i)
       
       with shared_lock:
           shared_list.extend(local_results)
   ```

3. **选择合适的并发模型**
   ```python
   # 根据任务类型选择
   import concurrent.futures
   
   def choose_concurrent_model(task_type, tasks):
       if task_type == "cpu_intensive":
           # 使用多进程
           with concurrent.futures.ProcessPoolExecutor() as executor:
               return executor.map(process_task, tasks)
       else:
           # 使用多线程
           with concurrent.futures.ThreadPoolExecutor() as executor:
               return executor.map(thread_task, tasks)
   ```

4. **监控和调试**
   ```python
   import threading
   import time
   
   class InstrumentedLock(threading.Lock):
       """带监控的锁"""
       def __init__(self):
           super().__init__()
           self.acquire_time = 0
           self.total_wait_time = 0
           self.acquire_count = 0
       
       def acquire(self, blocking=True, timeout=-1):
           start = time.time()
           result = super().acquire(blocking, timeout)
           if result:
               wait_time = time.time() - start
               self.acquire_time = time.time()
               self.total_wait_time += wait_time
               self.acquire_count += 1
           return result
       
       def get_stats(self):
           avg_wait = (self.total_wait_time / self.acquire_count 
                      if self.acquire_count > 0 else 0)
           return {
               'acquire_count': self.acquire_count,
               'total_wait_time': self.total_wait_time,
               'average_wait_time': avg_wait
           }
   ```

### 10.4 测试多线程代码

1. **单元测试**
   ```python
   import unittest
   import threading
   
   class TestThreadSafety(unittest.TestCase):
       def test_counter_thread_safety(self):
           from mymodule import AtomicCounter
           
           counter = AtomicCounter()
           threads = []
           
           def increment():
               for _ in range(1000):
                   counter.increment()
           
           # 创建多个线程
           for _ in range(10):
               thread = threading.Thread(target=increment)
               threads.append(thread)
               thread.start()
           
           # 等待所有线程
           for thread in threads:
               thread.join()
           
           # 验证结果
           self.assertEqual(counter.get_value(), 10 * 1000)
   
   if __name__ == "__main__":
       unittest.main()
   ```

2. **压力测试**
   ```python
   def stress_test(concurrent_class, duration=10):
       """压力测试并发类"""
       import time
       import random
       
       instance = concurrent_class()
       stop_event = threading.Event()
       error_count = 0
       
       def worker():
           nonlocal error_count
           while not stop_event.is_set():
               try:
                   # 执行随机操作
                   op = random.choice(['read', 'write', 'update'])
                   if op == 'read':
                       instance.get_value()
                   elif op == 'write':
                       instance.set_value(random.random())
                   else:
                       instance.update_value()
               except Exception as e:
                   error_count += 1
       
       # 启动工作线程
       threads = []
       for _ in range(20):
           thread = threading.Thread(target=worker)
           threads.append(thread)
           thread.start()
       
       # 运行指定时间
       time.sleep(duration)
       stop_event.set()
       
       # 等待线程结束
       for thread in threads:
           thread.join()
       
       print(f"Stress test completed: {error_count} errors")
       return error_count == 0
   ```

## 11. 总结

Python 线程编程通过 `threading` 模块提供了强大的并发能力，特别适合 I/O 密集型任务。关键要点包括：

1. **理解 GIL**：全局解释器锁限制了 CPU 密集型任务的并行性
2. **线程创建**：使用 `Thread` 类、继承或线程池
3. **同步机制**：锁、RLock、信号量、事件、条件变量、屏障
4. **线程安全**：避免竞态条件、死锁、活锁、饥饿
5. **线程间通信**：队列、共享变量、条件变量、线程局部存储
6. **线程池**：使用 `concurrent.futures` 或自定义实现
7. **实际应用**：文件下载、Web 请求、数据处理管道
8. **高级主题**：GIL 原理、与异步编程结合、原子操作、调试分析
9. **最佳实践**：合理设置线程数、使用线程安全结构、避免常见陷阱

通过掌握这些知识，你可以构建高性能、响应迅速的 Python 应用程序。记住，多线程不是万能的，需要根据具体场景选择合适的并发模型（线程、进程、协程）。

---

**扩展阅读**：
- [Python 官方文档 - threading](https://docs.python.org/3/library/threading.html)
- [Python 并发编程实战](https://realpython.com/python-concurrency/)
- [Effective Python - 第7章：并发与并行](https://effectivepython.com/)
- [GIL 解析](https://realpython.com/python-gil/)